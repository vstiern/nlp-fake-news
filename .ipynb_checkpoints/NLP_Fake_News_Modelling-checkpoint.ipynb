{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Assignment\n",
    "**Authors**: Vilhelm Stiernstedt & Sharon Mar√≠n Salazar\n",
    "<br>\n",
    "**Date**: 20/05/2018\n",
    "\n",
    "### Description\n",
    "Classification problem of News Report (document) for classes (FAKE, REAL). Try text-related classifiers such as Naive Bayes, MaxEnt, SVM. Use NLTK+SKLearn, NLP Pre-processing, Classifiers and CV-evaluation.\n",
    "\n",
    "#### Dataset\n",
    "**fake_or_real_news_training:**\n",
    "- ID: ID of the tweet\n",
    "- Title: Title of the news report\n",
    "- Text: Textual content of the news report\n",
    "- Label: Target Variable [FAKE, REAL]\n",
    "- X1, X2 additional fields\n",
    "\n",
    "**fake_or_real_news_test:**\n",
    "- ID, title and text\n",
    "- Predict Label\n",
    "\n",
    "#### Advices\n",
    "- Take a look to the data\n",
    "- Try the pre-processing methodologies we have seen in class\n",
    "- TF-IDF seems to be better (but try it!)\n",
    "- N-grams pay the effort\n",
    "- Less than 90-92%? -> Try again\n",
    "\n",
    "#### Plan\n",
    "1. Variable analysis\n",
    "    - Features\n",
    "    - Other insight\n",
    "2. Data Processing\n",
    "    - Drop features\n",
    "    - Label\n",
    "3. Modelling\n",
    "    - Navie Bayes\n",
    "    - SGD\n",
    "    - SVM\n",
    "        - linear\n",
    "        - rgb\n",
    "        - poly\n",
    "        - sigmod\n",
    "4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.classify import MaxentClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import PipelineHelper # https://github.com/bmurauer/pipelinehelper/blob/master/pipelinehelper.py\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import warnings\n",
    "\n",
    "# download required nltk packages (NB. commented out)\n",
    "# nltk.download()\n",
    "\n",
    "# plot settings\n",
    "%matplotlib inline\n",
    "\n",
    "# pandas view settings -> see all contents of column\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# Warning settings -> suppress depreciation warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build pipeline with multiple models\n",
    "# https://github.com/bmurauer/pipelinehelper/blob/master/pipelinehelper.py\n",
    "\n",
    "from sklearn.base import TransformerMixin, BaseEstimator, ClassifierMixin\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "class PipelineHelper(BaseEstimator, TransformerMixin, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, available_models=None, selected_model=None, include_bypass=False):\n",
    "        self.include_bypass = include_bypass\n",
    "        self.selected_model = selected_model\n",
    "        # this is required for the clone operator used in gridsearch\n",
    "        if type(available_models) == dict:\n",
    "            self.available_models = available_models\n",
    "        # this is the case for constructing the helper initially\n",
    "        else:\n",
    "            # a string identifier is required for assigning parameters\n",
    "            self.available_models = {}\n",
    "            for (key, model) in available_models:\n",
    "                self.available_models[key] = model\n",
    "\n",
    "    def generate(self, param_dict={}):\n",
    "        per_model_parameters = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "        # collect parameters for each specified model\n",
    "        for k, values in param_dict.items():\n",
    "            model_name = k.split('__')[0]\n",
    "            param_name = k[len(model_name)+2:]  # might be nested\n",
    "            if model_name not in self.available_models:\n",
    "                raise Exception('no such model: {0}'.format(model_name))\n",
    "            per_model_parameters[model_name][param_name] = values\n",
    "\n",
    "        ret = []\n",
    "\n",
    "        # create instance for cartesion product of all available parameters for each model\n",
    "        for model_name, param_dict in per_model_parameters.items():\n",
    "            parameter_sets = (dict(zip(param_dict, x)) for x in itertools.product(*param_dict.values()))\n",
    "            for parameters in parameter_sets:\n",
    "                ret.append((model_name, parameters))\n",
    "\n",
    "        # for every model that has no specified parameters, add the default model\n",
    "        for model_name in self.available_models.keys():\n",
    "            if model_name not in per_model_parameters:\n",
    "                ret.append((model_name, dict()))\n",
    "\n",
    "        # check if the stage is to be bypassed as one configuration\n",
    "        if self.include_bypass:\n",
    "            ret.append((None, dict(), True))\n",
    "        return ret\n",
    "\n",
    "    def get_params(self, deep=False):\n",
    "        return {'available_models': self.available_models,\n",
    "                'selected_model': self.selected_model,\n",
    "                'include_bypass': self.include_bypass}\n",
    "\n",
    "    def set_params(self, selected_model, available_models=None, include_bypass=False):\n",
    "        include_bypass = len(selected_model) == 3 and selected_model[2]\n",
    "\n",
    "        if available_models:\n",
    "            self.available_models = available_models\n",
    "\n",
    "        if selected_model[0] is None and include_bypass:\n",
    "            self.selected_model = None\n",
    "            self.include_bypass = True\n",
    "        else:\n",
    "            if selected_model[0] not in self.available_models:\n",
    "                raise Exception('so such model available: {0}'.format(selected_model[0]))\n",
    "            self.selected_model = self.available_models[selected_model[0]]\n",
    "            self.selected_model.set_params(**selected_model[1])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.selected_model is None and not self.include_bypass:\n",
    "            raise Exception('no model was set')\n",
    "        elif self.selected_model is None:\n",
    "            # print('bypassing model for fitting, returning self')\n",
    "            return self\n",
    "        else:\n",
    "            # print('using model for fitting: ', self.selected_model.__class__.__name__)\n",
    "            return self.selected_model.fit(X, y)\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if self.selected_model is None and not self.include_bypass:\n",
    "            raise Exception('no model was set')\n",
    "        elif self.selected_model is None:\n",
    "            # print('bypassing model for transforming:')\n",
    "            # print(X[:10])\n",
    "            return X\n",
    "        else:\n",
    "            # print('using model for transforming: ', self.selected_model.__class__.__name__)\n",
    "            return self.selected_model.transform(X)\n",
    "\n",
    "    def predict(self, x):\n",
    "        if self.include_bypass:\n",
    "            raise Exception('bypassing classifier is not allowed')\n",
    "        if self.selected_model is None:\n",
    "            raise Exception('no model was set')\n",
    "        return self.selected_model.predict(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to data\n",
    "data_path = 'data/'\n",
    "\n",
    "# load test and train\n",
    "df_train = pd.read_csv(data_path+'training_clean.csv')\n",
    "df_test = pd.read_csv(data_path+'fake_or_real_news_test.csv')\n",
    "\n",
    "# set index\n",
    "df_train.set_index('ID', inplace=True)\n",
    "df_test.set_index('ID', inplace=True)\n",
    "\n",
    "# define combined df\n",
    "all_data = df_train.append(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing\n",
    "#### Stemmers & Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define count vectorizer for modelling (different parameter inputs will be given in modelling)\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# define Snowball stemmer (different parameter inputs will be given in modelling\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# define new vectorizer function with snowball stemmer\n",
    "class SnowballCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(SnowballCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([snowball_stemmer.stem(w) for w in analyzer(doc)])\n",
    "    \n",
    "# define new vectorizer function with Porter stemmer NLTK exten \n",
    "# (different parameter inputs will be given in modelling)\n",
    "porter_stemmer = PorterStemmer(mode='NLTK_EXTENSIONS')\n",
    "\n",
    "# define new vectorizer function with porter stemmer\n",
    "class PorterCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(PorterCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([porter_stemmer.stem(w) for w in analyzer(doc)])\n",
    "    \n",
    "# define lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# define new vectorizer function with stemmer\n",
    "class LemmatizerCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(LemmatizerCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([lemmatizer.lemmatize(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Pipeline 1\n",
    "In our first pipeline we will try our simple count vectorizer but also different type of stemmers with various inputs such as n-grams, remove stopword and convert to lowercase. We will also see if using tf-idf(Term Frequency times inverse document frequency) can increase our scroe along with some parameter tuning for our models. We hope that of these stemmers will be better than our baseline for all tested models. \n",
    "\n",
    "We will assess the following combinations:\n",
    "    - count vectorizer\n",
    "    - count vectorizer w. snowball stemmer\n",
    "    - count vectorizer w. porter stemmer\n",
    "    - count vectorizer w. lemmatizer\n",
    "    \n",
    "TF-IDF = If we want to reduce the weightage of more common words, we deploy our vectorizer into the TF-IDF transformer, which will assign more weight to less common words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create different feature subsets\n",
    "df_model = df_train.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save label\n",
    "label = df_train.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data and labels into train and validation 80/20\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(df_model, label,\n",
    "                                                                test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline (vectorizer, models)\n",
    "pipeline = Pipeline([('vect', PipelineHelper([\n",
    "                            ('counter', CountVectorizer()),\n",
    "                            ('snowball_stemmer', SnowballCountVectorizer()),\n",
    "                            ('porter_stemmer', PorterCountVectorizer()),\n",
    "                            ('lemmatizer', LemmatizerCountVectorizer()),\n",
    "                        ])),\n",
    "                     ('tf-idf', TfidfTransformer()),\n",
    "                     ('clf', PipelineHelper([\n",
    "                            ('sgd', SGDClassifier(n_iter=1000)),\n",
    "                            #('svm-lin', LinearSVC()),\n",
    "                            #('svm-ker', SVC()),\n",
    "                            ('multi_nb', MultinomialNB()),\n",
    "                        ])),\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "We will extend the model parameters and hope to imporve our score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipline parameters\n",
    "parameters = {'vect__selected_model': pipeline.named_steps['vect'].generate({\n",
    "                  'counter__ngram_range': [(1, 2), (1, 3), (1, 4)],\n",
    "                  'counter__stop_words': ('english', None),\n",
    "                  'counter__lowercase': (True, False),\n",
    "                  'snowball_stemmer__ngram_range': [(1, 2), (1, 3), (1, 4)],\n",
    "                  'snowball_stemmer__stop_words': ('english', None),\n",
    "                  'snowball_stemmer__lowercase': (True, False),\n",
    "                  'porter_stemmer__ngram_range': [(1, 2), (1, 3), (1, 4)],\n",
    "                  'porter_stemmer__stop_words': ('english', None),\n",
    "                  'porter_stemmer__lowercase': (True, False),\n",
    "                  'lemmatizer__ngram_range': [(1, 2), (1, 3), (1, 4)],\n",
    "                  'lemmatizer__stop_words': ('english', None),\n",
    "                  'lemmatizer__lowercase': (True, False),\n",
    "                }),\n",
    "              'tf-idf__use_idf': (True, False),\n",
    "              'clf__selected_model': pipeline.named_steps['clf'].generate({\n",
    "                    'sgd__alpha': (1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3),\n",
    "                    'sgd__loss': ('hinge', 'squared_hinge', 'log'),\n",
    "                    'sgd__l1_ratio': (0, 0.1, 0.25, 0.5, 0.75, 1.0),\n",
    "                #  'svm-lin__penalty': ('l1', 'l2'),\n",
    "                #  'svm-lin__loss': ('hinge', 'squared_hinge'),\n",
    "                #  'svm-lin__C': (0.1, 1, 10, 50, 100, 500, 1000),\n",
    "                #  'svm-ker__kernel': ('rbf', 'poly', 'sigmoid'),\n",
    "                #  'svm-ker__C': (0.1, 1, 10, 50, 100, 500, 1000),\n",
    "                    'multi_nb__alpha': (0.1, 0.25, 0.5, 0.75, 1)\n",
    "                })\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 1.0, 'loss': 'hinge', 'l1_ratio': 0.5}) \n",
      "[CV] vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 1.0, 'loss': 'hinge', 'l1_ratio': 0.5}) \n",
      "[CV] vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 1.0, 'loss': 'hinge', 'l1_ratio': 0.5}) \n",
      "[CV]  vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 1.0, 'loss': 'hinge', 'l1_ratio': 0.5}), score=0.507029053420806, total=11.4min\n",
      "[CV] vect__selected_model=('lemmatizer', {'ngram_range': (1, 4), 'stop_words': 'english', 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 0.0001, 'loss': 'squared_hinge', 'l1_ratio': 0.25}) \n",
      "[CV]  vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 1.0, 'loss': 'hinge', 'l1_ratio': 0.5}), score=0.4971857410881801, total=11.4min\n",
      "[CV] vect__selected_model=('lemmatizer', {'ngram_range': (1, 4), 'stop_words': 'english', 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 0.0001, 'loss': 'squared_hinge', 'l1_ratio': 0.25}) \n",
      "[CV]  vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 1.0, 'loss': 'hinge', 'l1_ratio': 0.5}), score=0.5562851782363978, total=11.4min\n",
      "[CV] vect__selected_model=('lemmatizer', {'ngram_range': (1, 4), 'stop_words': 'english', 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 0.0001, 'loss': 'squared_hinge', 'l1_ratio': 0.25}) \n",
      "[CV]  vect__selected_model=('lemmatizer', {'ngram_range': (1, 4), 'stop_words': 'english', 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 0.0001, 'loss': 'squared_hinge', 'l1_ratio': 0.25}), score=0.8078725398313027, total= 2.6min\n",
      "[CV] vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'hinge', 'l1_ratio': 0.25}) \n",
      "[CV]  vect__selected_model=('lemmatizer', {'ngram_range': (1, 4), 'stop_words': 'english', 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 0.0001, 'loss': 'squared_hinge', 'l1_ratio': 0.25}), score=0.7926829268292683, total= 2.6min\n",
      "[CV] vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'hinge', 'l1_ratio': 0.25}) \n",
      "[CV]  vect__selected_model=('lemmatizer', {'ngram_range': (1, 4), 'stop_words': 'english', 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 0.0001, 'loss': 'squared_hinge', 'l1_ratio': 0.25}), score=0.8095684803001876, total= 2.6min\n",
      "[CV] vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'hinge', 'l1_ratio': 0.25}) \n",
      "[CV]  vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'hinge', 'l1_ratio': 0.25}), score=0.7122774133083412, total= 4.4min\n",
      "[CV] vect__selected_model=('porter_stemmer', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 1000.0, 'loss': 'log', 'l1_ratio': 0.1}) \n",
      "[CV]  vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'hinge', 'l1_ratio': 0.25}), score=0.4971857410881801, total= 4.5min\n",
      "[CV] vect__selected_model=('porter_stemmer', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 1000.0, 'loss': 'log', 'l1_ratio': 0.1}) \n",
      "[CV]  vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'hinge', 'l1_ratio': 0.25}), score=0.4971857410881801, total= 4.4min\n",
      "[CV] vect__selected_model=('porter_stemmer', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 1000.0, 'loss': 'log', 'l1_ratio': 0.1}) \n",
      "[CV]  vect__selected_model=('porter_stemmer', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 1000.0, 'loss': 'log', 'l1_ratio': 0.1}), score=0.5023430178069354, total= 5.2min\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': None, 'lowercase': False}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'hinge', 'l1_ratio': 0}) \n",
      "[CV]  vect__selected_model=('porter_stemmer', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 1000.0, 'loss': 'log', 'l1_ratio': 0.1}), score=0.4971857410881801, total= 5.3min\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': None, 'lowercase': False}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'hinge', 'l1_ratio': 0}) \n",
      "[CV]  vect__selected_model=('porter_stemmer', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 1000.0, 'loss': 'log', 'l1_ratio': 0.1}), score=0.4971857410881801, total= 5.3min\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': None, 'lowercase': False}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'hinge', 'l1_ratio': 0}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': None, 'lowercase': False}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'hinge', 'l1_ratio': 0}), score=0.6719775070290535, total= 2.5min\n",
      "[CV] vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 2), 'stop_words': 'english', 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 100.0, 'loss': 'log', 'l1_ratio': 0.75}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': None, 'lowercase': False}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'hinge', 'l1_ratio': 0}), score=0.4971857410881801, total= 2.6min\n",
      "[CV] vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 2), 'stop_words': 'english', 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 100.0, 'loss': 'log', 'l1_ratio': 0.75}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': None, 'lowercase': False}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'hinge', 'l1_ratio': 0}), score=0.6303939962476548, total= 2.6min\n",
      "[CV] vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 2), 'stop_words': 'english', 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 100.0, 'loss': 'log', 'l1_ratio': 0.75}) \n",
      "[CV]  vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 2), 'stop_words': 'english', 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 100.0, 'loss': 'log', 'l1_ratio': 0.75}), score=0.49765698219306465, total= 2.5min\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': 'english', 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 100.0, 'loss': 'squared_hinge', 'l1_ratio': 0.5}) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 2), 'stop_words': 'english', 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 100.0, 'loss': 'log', 'l1_ratio': 0.75}), score=0.5028142589118199, total= 2.5min\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': 'english', 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 100.0, 'loss': 'squared_hinge', 'l1_ratio': 0.5}) \n",
      "[CV]  vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 2), 'stop_words': 'english', 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 100.0, 'loss': 'log', 'l1_ratio': 0.75}), score=0.5028142589118199, total= 2.6min\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': 'english', 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 100.0, 'loss': 'squared_hinge', 'l1_ratio': 0.5}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': 'english', 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 100.0, 'loss': 'squared_hinge', 'l1_ratio': 0.5}), score=0.5023430178069354, total= 1.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed: 37.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'hinge', 'l1_ratio': 1.0}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': 'english', 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 100.0, 'loss': 'squared_hinge', 'l1_ratio': 0.5}), score=0.4971857410881801, total= 1.3min\n",
      "[CV] vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'hinge', 'l1_ratio': 1.0}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': 'english', 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 100.0, 'loss': 'squared_hinge', 'l1_ratio': 0.5}), score=0.4971857410881801, total= 1.3min\n",
      "[CV] vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'hinge', 'l1_ratio': 1.0}) \n",
      "[CV]  vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'hinge', 'l1_ratio': 1.0}), score=0.4985941893158388, total=10.2min\n",
      "[CV] vect__selected_model=('porter_stemmer', {'ngram_range': (1, 2), 'stop_words': 'english', 'lowercase': False}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'squared_hinge', 'l1_ratio': 1.0}) \n",
      "[CV]  vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'hinge', 'l1_ratio': 1.0}), score=0.5056285178236398, total=10.4min\n",
      "[CV] vect__selected_model=('porter_stemmer', {'ngram_range': (1, 2), 'stop_words': 'english', 'lowercase': False}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'squared_hinge', 'l1_ratio': 1.0}) \n",
      "[CV]  vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'hinge', 'l1_ratio': 1.0}), score=0.5140712945590994, total=10.5min\n",
      "[CV] vect__selected_model=('porter_stemmer', {'ngram_range': (1, 2), 'stop_words': 'english', 'lowercase': False}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'squared_hinge', 'l1_ratio': 1.0}) \n",
      "[CV]  vect__selected_model=('porter_stemmer', {'ngram_range': (1, 2), 'stop_words': 'english', 'lowercase': False}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'squared_hinge', 'l1_ratio': 1.0}), score=0.8575445173383318, total= 3.4min\n",
      "[CV] vect__selected_model=('porter_stemmer', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 0.0001, 'loss': 'hinge', 'l1_ratio': 0.75}) \n",
      "[CV]  vect__selected_model=('porter_stemmer', {'ngram_range': (1, 2), 'stop_words': 'english', 'lowercase': False}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'squared_hinge', 'l1_ratio': 1.0}), score=0.8639774859287055, total= 3.4min\n",
      "[CV] vect__selected_model=('porter_stemmer', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 0.0001, 'loss': 'hinge', 'l1_ratio': 0.75}) \n",
      "[CV]  vect__selected_model=('porter_stemmer', {'ngram_range': (1, 2), 'stop_words': 'english', 'lowercase': False}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'squared_hinge', 'l1_ratio': 1.0}), score=0.8330206378986866, total= 3.4min\n",
      "[CV] vect__selected_model=('porter_stemmer', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 0.0001, 'loss': 'hinge', 'l1_ratio': 0.75}) \n",
      "[CV]  vect__selected_model=('porter_stemmer', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 0.0001, 'loss': 'hinge', 'l1_ratio': 0.75}), score=0.9081537019681349, total=11.8min\n",
      "[CV] vect__selected_model=('lemmatizer', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 1000.0, 'loss': 'log', 'l1_ratio': 0.25}) \n",
      "[CV]  vect__selected_model=('porter_stemmer', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 0.0001, 'loss': 'hinge', 'l1_ratio': 0.75}), score=0.9090056285178236, total=11.9min\n",
      "[CV] vect__selected_model=('lemmatizer', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 1000.0, 'loss': 'log', 'l1_ratio': 0.25}) \n",
      "[CV]  vect__selected_model=('porter_stemmer', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 0.0001, 'loss': 'hinge', 'l1_ratio': 0.75}), score=0.9043151969981238, total=12.0min\n",
      "[CV] vect__selected_model=('lemmatizer', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 1000.0, 'loss': 'log', 'l1_ratio': 0.25}) \n",
      "[CV]  vect__selected_model=('lemmatizer', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 1000.0, 'loss': 'log', 'l1_ratio': 0.25}), score=0.49765698219306465, total= 1.5min\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.01, 'loss': 'squared_hinge', 'l1_ratio': 0.75}) \n",
      "[CV]  vect__selected_model=('lemmatizer', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 1000.0, 'loss': 'log', 'l1_ratio': 0.25}), score=0.4971857410881801, total= 1.7min\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.01, 'loss': 'squared_hinge', 'l1_ratio': 0.75}) \n",
      "[CV]  vect__selected_model=('lemmatizer', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 1000.0, 'loss': 'log', 'l1_ratio': 0.25}), score=0.4971857410881801, total= 1.7min\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.01, 'loss': 'squared_hinge', 'l1_ratio': 0.75}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.01, 'loss': 'squared_hinge', 'l1_ratio': 0.75}), score=0.7938144329896907, total= 2.7min\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'hinge', 'l1_ratio': 0.75}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.01, 'loss': 'squared_hinge', 'l1_ratio': 0.75}), score=0.7673545966228893, total= 3.1min\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'hinge', 'l1_ratio': 0.75}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.01, 'loss': 'squared_hinge', 'l1_ratio': 0.75}), score=0.798311444652908, total= 3.1min\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'hinge', 'l1_ratio': 0.75}) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'hinge', 'l1_ratio': 0.75}), score=0.49765698219306465, total= 3.0min\n",
      "[CV] vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 4), 'stop_words': 'english', 'lowercase': True}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 100.0, 'loss': 'log', 'l1_ratio': 0.1}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'hinge', 'l1_ratio': 0.75}), score=0.7345215759849906, total= 2.9min\n",
      "[CV] vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 4), 'stop_words': 'english', 'lowercase': True}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 100.0, 'loss': 'log', 'l1_ratio': 0.1}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 0.1, 'loss': 'hinge', 'l1_ratio': 0.75}), score=0.698874296435272, total= 2.9min\n",
      "[CV] vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 4), 'stop_words': 'english', 'lowercase': True}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 100.0, 'loss': 'log', 'l1_ratio': 0.1}) \n",
      "[CV]  vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 4), 'stop_words': 'english', 'lowercase': True}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 100.0, 'loss': 'log', 'l1_ratio': 0.1}), score=0.5023430178069354, total= 5.5min\n",
      "[CV] vect__selected_model=('porter_stemmer', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': False}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 100.0, 'loss': 'squared_hinge', 'l1_ratio': 0.75}) \n",
      "[CV]  vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 4), 'stop_words': 'english', 'lowercase': True}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 100.0, 'loss': 'log', 'l1_ratio': 0.1}), score=0.4971857410881801, total= 5.6min\n",
      "[CV] vect__selected_model=('porter_stemmer', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': False}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 100.0, 'loss': 'squared_hinge', 'l1_ratio': 0.75}) \n",
      "[CV]  vect__selected_model=('snowball_stemmer', {'ngram_range': (1, 4), 'stop_words': 'english', 'lowercase': True}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 100.0, 'loss': 'log', 'l1_ratio': 0.1}), score=0.4971857410881801, total= 5.7min\n",
      "[CV] vect__selected_model=('porter_stemmer', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': False}), tf-idf__use_idf=True, clf__selected_model=('sgd', {'alpha': 100.0, 'loss': 'squared_hinge', 'l1_ratio': 0.75}) \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-206534a7fcf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# fit model based\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mrscv_clf_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrscv_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# get best score from CV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define random search grid with cv\n",
    "rscv_clf = RandomizedSearchCV(estimator=pipeline, verbose=4,\n",
    "                              param_distributions=parameters,\n",
    "                              n_jobs=3, n_iter=20, cv=3, \n",
    "                              random_state=42)\n",
    "\n",
    "# fit model based\n",
    "rscv_clf_mod = rscv_clf.fit(x_train, y_train)\n",
    "\n",
    "# get best score from CV\n",
    "rscv_clf_mod.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__selected_model': ('svm-ker', {'C': 10000.0, 'kernel': 'rbf'}),\n",
       " 'vect__selected_model': ('porter_stemmer',\n",
       "  {'lowercase': True, 'ngram_range': (1, 4), 'stop_words': None})}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get parameters for best score from CV\n",
    "rscv_clf_mod.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       FAKE       0.86      0.94      0.90       383\n",
      "       REAL       0.94      0.86      0.90       417\n",
      "\n",
      "avg / total       0.90      0.90      0.90       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "rscv_clf_pred = rscv_clf_mod.best_estimator_.predict(x_validation)\n",
    "\n",
    "# model evaluation\n",
    "print(metrics.classification_report(y_validation, rscv_clf_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Pipeline 2\n",
    "Our second pipeline will introduce an tf-idf to see if some of our models improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset for orginal text\n",
    "df_model = df_train['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save label\n",
    "label = df_train.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data and labels into train and validation 80/20\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(df_model, label,\n",
    "                                                                test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline (vectorizer, models)\n",
    "pipeline = Pipeline([('vect', PipelineHelper([\n",
    "                            ('counter', CountVectorizer()),\n",
    "                            ('snowball_stemmer', SnowballCountVectorizer()),\n",
    "                            ('porter_stemmer', PorterCountVectorizer()),\n",
    "                            ('lemmatizer', LemmatizerCountVectorizer()),\n",
    "                        ])),\n",
    "                     ('tf-idf', TfidfTransformer()),\n",
    "                     ('clf', PipelineHelper([\n",
    "                            ('svm', SGDClassifier()),\n",
    "                            ('multi_nb', MultinomialNB()),\n",
    "                        ])),\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "We will extend the model parameters and hope to imporve our score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipline parameters\n",
    "parameters = {'vect__selected_model': pipeline.named_steps['vect'].generate({\n",
    "                  'counter__ngram_range': [(1, 1), (1, 2)],\n",
    "                  'counter__stop_words': ('english', None),\n",
    "                  'counter__lowercase': (True, False),\n",
    "                  'snowball_stemmer__ngram_range': [(1, 1), (1, 2)],\n",
    "                  'snowball_stemmer__stop_words': ('english', None),\n",
    "                  'snowball_stemmer__lowercase': (True, False),\n",
    "                  'porter_stemmer__ngram_range': [(1, 1), (1, 2)],\n",
    "                  'porter_stemmer__stop_words': ('english', None),\n",
    "                  'porter_stemmer__lowercase': (True, False),\n",
    "                  'lemmatizer__ngram_range': [(1, 1), (1, 2)],\n",
    "                  'lemmatizer__stop_words': ('english', None),\n",
    "                  'lemmatizer__lowercase': (True, False),\n",
    "                }),\n",
    "              'tf-idf__use_idf': (True, False),\n",
    "              'clf__selected_model': pipeline.named_steps['clf'].generate({\n",
    "                  'svm__alpha': (0.5, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6),\n",
    "                  'svm__loss': ('hinge', 'squared_hinge'),\n",
    "                  'svm__l1_ratio': (0, 0.1, 0.25, 0.5, 0.75, 1.0),\n",
    "                  'multi_nb__alpha': (0.1, 0.25, 0.5, 0.75, 1)\n",
    "                })\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:   31.2s\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "[Parallel(n_jobs=3)]: Done  90 out of  90 | elapsed:   55.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8227571115973742"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define random search grid with cv\n",
    "rscv_clf = RandomizedSearchCV(estimator=pipeline, verbose=1,\n",
    "                              param_distributions=parameters,\n",
    "                              n_jobs=3, n_iter=30, cv=3, \n",
    "                              random_state=42)\n",
    "\n",
    "# fit model based\n",
    "rscv_clf_mod = rscv_clf.fit(x_train, y_train)\n",
    "\n",
    "# get best score from CV\n",
    "rscv_clf_mod.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__selected_model': ('svm',\n",
       "  {'alpha': 0.01, 'l1_ratio': 0.25, 'loss': 'hinge'}),\n",
       " 'vect__selected_model': ('lemmatizer',\n",
       "  {'lowercase': True, 'ngram_range': (1, 2), 'stop_words': 'english'})}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get parameters for best score from CV\n",
    "rscv_clf_mod.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       FAKE       0.89      0.92      0.90       383\n",
      "       REAL       0.93      0.89      0.91       417\n",
      "\n",
      "avg / total       0.91      0.91      0.91       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "rscv_clf_pred = rscv_clf_mod.best_estimator_.predict(x_validation)\n",
    "\n",
    "# model evaluation\n",
    "print(metrics.classification_report(y_validation, rscv_clf_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
