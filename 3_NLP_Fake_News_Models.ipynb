{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Assignment\n",
    "**Authors**: Vilhelm Stiernstedt, Sharon Mar√≠n Salazar & Andrea Tondella\n",
    "<br>\n",
    "**Date**: 26/05/2018\n",
    "\n",
    "#### Bag of Words Approaches\n",
    "In this section we will import our cleaned data from the analysis section and explore various models by setting up pipelines with different:\n",
    "- vectorizers\n",
    "- stemmers\n",
    "- lemmatizers\n",
    "- tf-idfs\n",
    "\n",
    "#### Models Optimization\n",
    "We will try to optimize the same models as establish in our baselines test in the previous section by trying hyperparameter optimining via a random grid search. As always we seek to split our data into training and validation sets, along with using cross validation to avoid overfitting. Models to try:\n",
    "- Navie Bayes\n",
    "- SGD\n",
    "- SVM\n",
    "    - linear\n",
    "    - rgb\n",
    "    - poly (removed due to heavy computations and inferior results)\n",
    "    - sigmod (removed due to heavy computations and inferior results)\n",
    "\n",
    "#### Evalutaion\n",
    "We will evaluate our models by trying to: \n",
    "- produce models that acheive high average accuracy\n",
    "- produce models that score high in either Fake or Real \n",
    "\n",
    "By doing so we hope to later use stacking/ensamble that can bring our models together and achieve a higher prediction score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general libs\n",
    "import warnings\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# nltk libs\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "# download required nltk packages (NB. commented out)\n",
    "# nltk.download()\n",
    "\n",
    "# sklearn libs\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# plot settings\n",
    "%matplotlib inline\n",
    "\n",
    "# pandas view settings -> see all contents of column\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# Warning settings -> suppress depreciation warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "# Warning settings -> suppress future warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build pipeline with multiple models\n",
    "# https://github.com/bmurauer/pipelinehelper/blob/master/pipelinehelper.py\n",
    "\n",
    "from sklearn.base import TransformerMixin, BaseEstimator, ClassifierMixin\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "class PipelineHelper(BaseEstimator, TransformerMixin, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, available_models=None, selected_model=None, include_bypass=False):\n",
    "        self.include_bypass = include_bypass\n",
    "        self.selected_model = selected_model\n",
    "        # this is required for the clone operator used in gridsearch\n",
    "        if type(available_models) == dict:\n",
    "            self.available_models = available_models\n",
    "        # this is the case for constructing the helper initially\n",
    "        else:\n",
    "            # a string identifier is required for assigning parameters\n",
    "            self.available_models = {}\n",
    "            for (key, model) in available_models:\n",
    "                self.available_models[key] = model\n",
    "\n",
    "    def generate(self, param_dict={}):\n",
    "        per_model_parameters = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "        # collect parameters for each specified model\n",
    "        for k, values in param_dict.items():\n",
    "            model_name = k.split('__')[0]\n",
    "            param_name = k[len(model_name)+2:]  # might be nested\n",
    "            if model_name not in self.available_models:\n",
    "                raise Exception('no such model: {0}'.format(model_name))\n",
    "            per_model_parameters[model_name][param_name] = values\n",
    "\n",
    "        ret = []\n",
    "\n",
    "        # create instance for cartesion product of all available parameters for each model\n",
    "        for model_name, param_dict in per_model_parameters.items():\n",
    "            parameter_sets = (dict(zip(param_dict, x)) for x in itertools.product(*param_dict.values()))\n",
    "            for parameters in parameter_sets:\n",
    "                ret.append((model_name, parameters))\n",
    "\n",
    "        # for every model that has no specified parameters, add the default model\n",
    "        for model_name in self.available_models.keys():\n",
    "            if model_name not in per_model_parameters:\n",
    "                ret.append((model_name, dict()))\n",
    "\n",
    "        # check if the stage is to be bypassed as one configuration\n",
    "        if self.include_bypass:\n",
    "            ret.append((None, dict(), True))\n",
    "        return ret\n",
    "\n",
    "    def get_params(self, deep=False):\n",
    "        return {'available_models': self.available_models,\n",
    "                'selected_model': self.selected_model,\n",
    "                'include_bypass': self.include_bypass}\n",
    "\n",
    "    def set_params(self, selected_model, available_models=None, include_bypass=False):\n",
    "        include_bypass = len(selected_model) == 3 and selected_model[2]\n",
    "\n",
    "        if available_models:\n",
    "            self.available_models = available_models\n",
    "\n",
    "        if selected_model[0] is None and include_bypass:\n",
    "            self.selected_model = None\n",
    "            self.include_bypass = True\n",
    "        else:\n",
    "            if selected_model[0] not in self.available_models:\n",
    "                raise Exception('so such model available: {0}'.format(selected_model[0]))\n",
    "            self.selected_model = self.available_models[selected_model[0]]\n",
    "            self.selected_model.set_params(**selected_model[1])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.selected_model is None and not self.include_bypass:\n",
    "            raise Exception('no model was set')\n",
    "        elif self.selected_model is None:\n",
    "            # print('bypassing model for fitting, returning self')\n",
    "            return self\n",
    "        else:\n",
    "            # print('using model for fitting: ', self.selected_model.__class__.__name__)\n",
    "            return self.selected_model.fit(X, y)\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if self.selected_model is None and not self.include_bypass:\n",
    "            raise Exception('no model was set')\n",
    "        elif self.selected_model is None:\n",
    "            # print('bypassing model for transforming:')\n",
    "            # print(X[:10])\n",
    "            return X\n",
    "        else:\n",
    "            # print('using model for transforming: ', self.selected_model.__class__.__name__)\n",
    "            return self.selected_model.transform(X)\n",
    "\n",
    "    def predict(self, x):\n",
    "        if self.include_bypass:\n",
    "            raise Exception('bypassing classifier is not allowed')\n",
    "        if self.selected_model is None:\n",
    "            raise Exception('no model was set')\n",
    "        return self.selected_model.predict(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to data\n",
    "data_path = 'data/'\n",
    "\n",
    "# load test and train\n",
    "df_train = pd.read_csv(data_path+'training_clean.csv')\n",
    "df_test = pd.read_csv(data_path+'fake_or_real_news_test.csv')\n",
    "\n",
    "# set index\n",
    "df_train.set_index('ID', inplace=True)\n",
    "df_test.set_index('ID', inplace=True)\n",
    "\n",
    "# define combined df\n",
    "all_data = df_train.append(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing\n",
    "#### Stemmers & Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define count vectorizer for modelling (different parameter inputs will be given in modelling)\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# define Snowball stemmer (different parameter inputs will be given in modelling\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# define new vectorizer function with snowball stemmer\n",
    "class SnowballCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(SnowballCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([snowball_stemmer.stem(w) for w in analyzer(doc)])\n",
    "    \n",
    "# define new vectorizer function with Porter stemmer NLTK exten \n",
    "# (different parameter inputs will be given in modelling)\n",
    "porter_stemmer = PorterStemmer(mode='NLTK_EXTENSIONS')\n",
    "\n",
    "# define new vectorizer function with porter stemmer\n",
    "class PorterCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(PorterCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([porter_stemmer.stem(w) for w in analyzer(doc)])\n",
    "    \n",
    "# define lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# define new vectorizer function with stemmer\n",
    "class LemmatizerCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(LemmatizerCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([lemmatizer.lemmatize(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Pipeline 1\n",
    "In our first pipeline we will try our simple count vectorizer but also different type of stemmers with various inputs such as n-grams, remove stopword and convert to lowercase. We will also see if using tf-idf(Term Frequency times inverse document frequency) can increase our scroe along with some parameter tuning for our models. We hope that of these stemmers will be better than our baseline for all tested models. \n",
    "\n",
    "We will assess the following combinations:\n",
    "    - count vectorizer\n",
    "    - count vectorizer w. snowball stemmer\n",
    "    - count vectorizer w. porter stemmer\n",
    "    - count vectorizer w. lemmatizer\n",
    "    \n",
    "TF-IDF = If we want to reduce the weightage of more common words, we deploy our vectorizer into the TF-IDF transformer, which will assign more weight to less common words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create different feature subsets\n",
    "x = df_train.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save label\n",
    "y = df_train.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data and labels into train and validation 80/20 -> not used \n",
    "# -> causes too much data leakage -> model overfits\n",
    "#x_train, x_validation, y_train, y_validation = train_test_split(x, y,\n",
    "#                                                                test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline (vectorizer, models)\n",
    "pipeline = Pipeline([('vect', PipelineHelper([\n",
    "                            ('counter', CountVectorizer()),\n",
    "                            #('snowball_stemmer', SnowballCountVectorizer()),\n",
    "                            #('porter_stemmer', PorterCountVectorizer()),\n",
    "                            #('lemmatizer', LemmatizerCountVectorizer()),\n",
    "                        ])),\n",
    "                     ('clf', PipelineHelper([\n",
    "                            #('sgd', SGDClassifier()),\n",
    "                            #('svm-lin', LinearSVC()),\n",
    "                            #('svm-ker', SVC()),\n",
    "                            ('multi_nb', MultinomialNB()),\n",
    "                        ])),\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "We will extend the model parameters and hope to imporve our score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipline parameters\n",
    "parameters = {'vect__selected_model': pipeline.named_steps['vect'].generate({\n",
    "                  'counter__ngram_range': [(1, 2), (1, 3)],\n",
    "                  'counter__stop_words': ('english', None),\n",
    "                  'counter__lowercase': (True, False),\n",
    "                  'counter__max_df': (0.25, 0.5, 0.75, 1),\n",
    "                  #'snowball_stemmer__ngram_range': [(1, 2), (1, 3)],\n",
    "                  #'snowball_stemmer__stop_words': ('english', None),\n",
    "                  #'snowball_stemmer__lowercase': (True, False),\n",
    "                  #'snowball_stemmer__max_df': (0.5, 0.75, 1),\n",
    "                  #'porter_stemmer__ngram_range': [(1, 2), (1, 3)],\n",
    "                  #'porter_stemmer__stop_words': ('english', None),\n",
    "                  #'porter_stemmer__max_df': (0.5, 0.75, 0.1),\n",
    "                  #'porter_stemmer__lowercase': (True, False),\n",
    "                  #'lemmatizer__ngram_range': [(1, 2), (1, 3)],\n",
    "                  #'lemmatizer__stop_words': ('english', None),\n",
    "                  #'lemmatizer__lowercase': (True, False),\n",
    "                  #'lemmatizer__max_df': (0.5, 0.75, 1),\n",
    "                }),\n",
    "              'clf__selected_model': pipeline.named_steps['clf'].generate({\n",
    "                    #'sgd__alpha': (1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3),\n",
    "                    #'sgd__loss': ('hinge', 'squared_hinge', 'log'),\n",
    "                    #'sgd__l1_ratio': (0, 0.1, 0.25, 0.5, 0.75, 1.0),\n",
    "                    #'svm-lin__penalty': ('l1', 'l2'),\n",
    "                    #'svm-lin__loss': ('hinge', 'squared_hinge'),\n",
    "                    #'svm-lin__C': (0.1, 1, 10, 50, 100, 500, 1000),\n",
    "                    #'svm-ker__kernel': ('rbf', 'poly', 'sigmoid'),\n",
    "                    #'svm-ker__C': (0.1, 1, 10, 50, 100, 500, 1000),\n",
    "                    'multi_nb__alpha': (0.1, 0.25, 0.5)\n",
    "                })\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': None, 'lowercase': False, 'max_df': 0.75}), clf__selected_model=('multi_nb', {'alpha': 0.1}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': None, 'lowercase': False, 'max_df': 0.75}), clf__selected_model=('multi_nb', {'alpha': 0.1}), score=0.9115442278860569, total=  33.4s\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': None, 'lowercase': False, 'max_df': 0.75}), clf__selected_model=('multi_nb', {'alpha': 0.1}) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   41.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': None, 'lowercase': False, 'max_df': 0.75}), clf__selected_model=('multi_nb', {'alpha': 0.1}), score=0.913728432108027, total=  32.4s\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': None, 'lowercase': False, 'max_df': 0.75}), clf__selected_model=('multi_nb', {'alpha': 0.1}) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': None, 'lowercase': False, 'max_df': 0.75}), clf__selected_model=('multi_nb', {'alpha': 0.1}), score=0.8888888888888888, total=  32.4s\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': None, 'lowercase': False, 'max_df': 0.75}), clf__selected_model=('multi_nb', {'alpha': 0.25}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': None, 'lowercase': False, 'max_df': 0.75}), clf__selected_model=('multi_nb', {'alpha': 0.25}), score=0.9070464767616192, total=  34.3s\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': None, 'lowercase': False, 'max_df': 0.75}), clf__selected_model=('multi_nb', {'alpha': 0.25}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': None, 'lowercase': False, 'max_df': 0.75}), clf__selected_model=('multi_nb', {'alpha': 0.25}), score=0.9062265566391597, total=  32.7s\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': None, 'lowercase': False, 'max_df': 0.75}), clf__selected_model=('multi_nb', {'alpha': 0.25}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': None, 'lowercase': False, 'max_df': 0.75}), clf__selected_model=('multi_nb', {'alpha': 0.25}), score=0.8851351351351351, total=  34.1s\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': None, 'lowercase': False, 'max_df': 0.5}), clf__selected_model=('multi_nb', {'alpha': 0.25}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': None, 'lowercase': False, 'max_df': 0.5}), clf__selected_model=('multi_nb', {'alpha': 0.25}), score=0.9092953523238381, total=  30.5s\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': None, 'lowercase': False, 'max_df': 0.5}), clf__selected_model=('multi_nb', {'alpha': 0.25}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': None, 'lowercase': False, 'max_df': 0.5}), clf__selected_model=('multi_nb', {'alpha': 0.25}), score=0.9099774943735934, total=  28.9s\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': None, 'lowercase': False, 'max_df': 0.5}), clf__selected_model=('multi_nb', {'alpha': 0.25}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': None, 'lowercase': False, 'max_df': 0.5}), clf__selected_model=('multi_nb', {'alpha': 0.25}), score=0.8881381381381381, total=  28.4s\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': 'english', 'lowercase': True, 'max_df': 0.75}), clf__selected_model=('multi_nb', {'alpha': 0.25}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': 'english', 'lowercase': True, 'max_df': 0.75}), clf__selected_model=('multi_nb', {'alpha': 0.25}), score=0.9055472263868066, total=  18.2s\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': 'english', 'lowercase': True, 'max_df': 0.75}), clf__selected_model=('multi_nb', {'alpha': 0.25}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': 'english', 'lowercase': True, 'max_df': 0.75}), clf__selected_model=('multi_nb', {'alpha': 0.25}), score=0.9107276819204801, total=  18.4s\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': 'english', 'lowercase': True, 'max_df': 0.75}), clf__selected_model=('multi_nb', {'alpha': 0.25}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 3), 'stop_words': 'english', 'lowercase': True, 'max_df': 0.75}), clf__selected_model=('multi_nb', {'alpha': 0.25}), score=0.8896396396396397, total=  18.5s\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True, 'max_df': 0.75}), clf__selected_model=('multi_nb', {'alpha': 0.25}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True, 'max_df': 0.75}), clf__selected_model=('multi_nb', {'alpha': 0.25}), score=0.8935532233883059, total=  14.4s\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True, 'max_df': 0.75}), clf__selected_model=('multi_nb', {'alpha': 0.25}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True, 'max_df': 0.75}), clf__selected_model=('multi_nb', {'alpha': 0.25}), score=0.9047261815453863, total=  14.7s\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True, 'max_df': 0.75}), clf__selected_model=('multi_nb', {'alpha': 0.25}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True, 'max_df': 0.75}), clf__selected_model=('multi_nb', {'alpha': 0.25}), score=0.8806306306306306, total=  14.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:  8.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9047261815453863"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define random search grid with cv\n",
    "rscv_clf = RandomizedSearchCV(estimator=pipeline, verbose=3,\n",
    "                              param_distributions=parameters,\n",
    "                              n_jobs=1, n_iter=5, cv=3, \n",
    "                              random_state=42)\n",
    "\n",
    "# fit model\n",
    "rscv_clf_mod = rscv_clf.fit(x, y)\n",
    "\n",
    "# get best score from CV\n",
    "rscv_clf_mod.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vect__selected_model': ('counter',\n",
       "  {'ngram_range': (1, 3),\n",
       "   'stop_words': None,\n",
       "   'lowercase': False,\n",
       "   'max_df': 0.75}),\n",
       " 'clf__selected_model': ('multi_nb', {'alpha': 0.1})}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get parameters for best score from CV\n",
    "rscv_clf_mod.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions -> no validation at this stage -> data leakage -> overfitting of model\n",
    "#rscv_clf_pred = rscv_clf_mod.best_estimator_.predict(x_validation)\n",
    "\n",
    "# model evaluation\n",
    "#print(metrics.classification_report(y_validation, rscv_clf_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. {'clf__selected_model': ('multi_nb', {'alpha': 0.25}),\n",
    " 'vect__selected_model': ('counter',\n",
    "  {'lowercase': True, 'ngram_range': (1, 3), 'stop_words': None})}\n",
    "\n",
    "precision  /  recall / f1-score  / support\n",
    "\n",
    "FAKE    /   0.96  /    0.89   /   0.92    /   383\n",
    "<br>\n",
    "REAL    /   0.90  /    0.96   /   0.93   /    417\n",
    "\n",
    "avg / total    /   0.93   /   0.93   /   0.93    /   800\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "\n",
    "2. {'clf__selected_model': ('sgd', {'alpha': 0.1, 'l1_ratio': 0.75, 'loss': 'hinge', 'n_iter': 800}) 'vect__selected_model': ('snowball_stemmer', {'lowercase': True, 'ngram_range': (1, 2), 'stop_words': None})}\n",
    "\n",
    "precision  /  recall  f1-score   support\n",
    "\n",
    "FAKE    /   0.87    /  0.95   /   0.91   /    383\n",
    "<br>\n",
    "REAL    /   0.95   /   0.87   /   0.91   /    417\n",
    "\n",
    "avg / total    /   0.91    /  0.91   /   0.91   /    800\n",
    "\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "3. {'clf__selected_model': ('svm-ker', {'C': 10000.0, 'kernel': 'rbf'}),\n",
    " 'vect__selected_model': ('porter_stemmer',\n",
    "  {'lowercase': True, 'ngram_range': (1, 4), 'stop_words': None})}\n",
    "\n",
    "\n",
    "precision    recall  f1-score   support\n",
    "\n",
    "FAKE    /   0.86    /  0.94   /  0.90     /  383\n",
    "<br>\n",
    "REAL    /   0.94    /  0.86   /   0.90    /   417\n",
    "\n",
    "avg / total   /  0.90   /   0.90   /   0.90    /   800\n",
    "\n",
    "----------------------------------------------------------------------\n",
    "4. svm-kernel (cv=3, n-iter=20): \n",
    "- vect__selected_model=('counter', {'ngram_range': (1, 3),\n",
    " 'stop_words': None, 'lowercase': False}), clf__selected_model=('svm-ker',\n",
    "  {'kernel': 'rbf', 'C': 1000.0}), score=0.8818011257035647, total= 1.3min\n",
    "\n",
    "- vect__selected_model=('porter_stemmer', {'ngram_range': (1, 4),\n",
    "'stop_words': None, 'lowercase': True}), clf__selected_model=('svm-ker',\n",
    "{'kernel': 'rbf', 'C': 10000.0}), score=0.8958724202626641, total=12.4min\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Pipeline 2\n",
    "Our second pipeline will introduce an tf-idf to see if some of our models improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset for orginal text\n",
    "x = df_train.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save label\n",
    "y = df_train.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data and labels into train and validation 80/20 -> not used due to too much data leakage\n",
    "#x_train, x_validation, y_train, y_validation = train_test_split(x, y,\n",
    "#                                                                test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline (vectorizer, models)\n",
    "pipeline_2 = Pipeline([('vect', PipelineHelper([\n",
    "                            ('counter', CountVectorizer()),\n",
    "                            #('snowball_stemmer', SnowballCountVectorizer()),\n",
    "                            #('porter_stemmer', PorterCountVectorizer()),\n",
    "                            ('lemmatizer', LemmatizerCountVectorizer()),\n",
    "                        ])),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', PipelineHelper([\n",
    "                            #('sgd', SGDClassifier()),\n",
    "                            #('svm-lin', LinearSVC()),\n",
    "                            #('svm-ker', SVC()),\n",
    "                            ('multi_nb', MultinomialNB()),\n",
    "                        ])),\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "We will extend the model parameters and hope to imporve our score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipline parameters\n",
    "parameters_2 = {'vect__selected_model': pipeline.named_steps['vect'].generate({\n",
    "                  'counter__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                  'counter__stop_words': ('english', None),\n",
    "                  'counter__lowercase': (True, False),\n",
    "                  'counter__max_df': (0.5, 0.75, 1),\n",
    "                  #'snowball_stemmer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                  #'snowball_stemmer__stop_words': ('english', None),\n",
    "                  #'snowball_stemmer__lowercase': (True, False),\n",
    "                  #'snowball_stemmer__max_df': (0.5, 0.75, 1),\n",
    "                  #'porter_stemmer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                  #'porter_stemmer__stop_words': ('english', None),\n",
    "                  #'porter_stemmer__lowercase': (True, False),\n",
    "                  #'porter_stemmer__max_df': (0.5, 0.75, 0. 1),\n",
    "                  'lemmatizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                  'lemmatizer__stop_words': ('english', None),\n",
    "                  'lemmatizer__lowercase': (True, False),\n",
    "                  'lemmatizer__max_df': (0.5, 0.75, 1),\n",
    "                }),\n",
    "              'tfidf__norm': ('l1', 'l2'),\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'tfidf__smooth_idf': (True, False),\n",
    "              'clf__selected_model': pipeline.named_steps['clf'].generate({\n",
    "                  #'sgd__alpha': (0.5, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6),\n",
    "                  #'sdg__loss': ('hinge', 'squared_hinge'),\n",
    "                  #'sdg__l1_ratio': (0, 0.1, 0.25, 0.5, 0.75, 1.0),\n",
    "                  'multi_nb__alpha': (0.01, 0.1, 0.25, 0.5)\n",
    "                })\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] vect__selected_model=('lemmatizer', {'ngram_range': (1, 1), 'stop_words': None, 'lowercase': False, 'max_df': 0.75}), tfidf__use_idf=False, tfidf__smooth_idf=False, tfidf__norm=l1, clf__selected_model=('multi_nb', {'alpha': 0.1}) \n",
      "[CV]  vect__selected_model=('lemmatizer', {'ngram_range': (1, 1), 'stop_words': None, 'lowercase': False, 'max_df': 0.75}), tfidf__use_idf=False, tfidf__smooth_idf=False, tfidf__norm=l1, clf__selected_model=('multi_nb', {'alpha': 0.1}), score=0.8185907046476761, total=  30.1s\n",
      "[CV] vect__selected_model=('lemmatizer', {'ngram_range': (1, 1), 'stop_words': None, 'lowercase': False, 'max_df': 0.75}), tfidf__use_idf=False, tfidf__smooth_idf=False, tfidf__norm=l1, clf__selected_model=('multi_nb', {'alpha': 0.1}) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   40.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vect__selected_model=('lemmatizer', {'ngram_range': (1, 1), 'stop_words': None, 'lowercase': False, 'max_df': 0.75}), tfidf__use_idf=False, tfidf__smooth_idf=False, tfidf__norm=l1, clf__selected_model=('multi_nb', {'alpha': 0.1}), score=0.8304576144036009, total=  26.2s\n",
      "[CV] vect__selected_model=('lemmatizer', {'ngram_range': (1, 1), 'stop_words': None, 'lowercase': False, 'max_df': 0.75}), tfidf__use_idf=False, tfidf__smooth_idf=False, tfidf__norm=l1, clf__selected_model=('multi_nb', {'alpha': 0.1}) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  vect__selected_model=('lemmatizer', {'ngram_range': (1, 1), 'stop_words': None, 'lowercase': False, 'max_df': 0.75}), tfidf__use_idf=False, tfidf__smooth_idf=False, tfidf__norm=l1, clf__selected_model=('multi_nb', {'alpha': 0.1}), score=0.8108108108108109, total=  25.7s\n",
      "[CV] vect__selected_model=('lemmatizer', {'ngram_range': (1, 1), 'stop_words': 'english', 'lowercase': True, 'max_df': 0.5}), tfidf__use_idf=False, tfidf__smooth_idf=True, tfidf__norm=l1, clf__selected_model=('multi_nb', {'alpha': 0.25}) \n",
      "[CV]  vect__selected_model=('lemmatizer', {'ngram_range': (1, 1), 'stop_words': 'english', 'lowercase': True, 'max_df': 0.5}), tfidf__use_idf=False, tfidf__smooth_idf=True, tfidf__norm=l1, clf__selected_model=('multi_nb', {'alpha': 0.25}), score=0.8298350824587706, total=  17.0s\n",
      "[CV] vect__selected_model=('lemmatizer', {'ngram_range': (1, 1), 'stop_words': 'english', 'lowercase': True, 'max_df': 0.5}), tfidf__use_idf=False, tfidf__smooth_idf=True, tfidf__norm=l1, clf__selected_model=('multi_nb', {'alpha': 0.25}) \n",
      "[CV]  vect__selected_model=('lemmatizer', {'ngram_range': (1, 1), 'stop_words': 'english', 'lowercase': True, 'max_df': 0.5}), tfidf__use_idf=False, tfidf__smooth_idf=True, tfidf__norm=l1, clf__selected_model=('multi_nb', {'alpha': 0.25}), score=0.8372093023255814, total=  16.6s\n",
      "[CV] vect__selected_model=('lemmatizer', {'ngram_range': (1, 1), 'stop_words': 'english', 'lowercase': True, 'max_df': 0.5}), tfidf__use_idf=False, tfidf__smooth_idf=True, tfidf__norm=l1, clf__selected_model=('multi_nb', {'alpha': 0.25}) \n",
      "[CV]  vect__selected_model=('lemmatizer', {'ngram_range': (1, 1), 'stop_words': 'english', 'lowercase': True, 'max_df': 0.5}), tfidf__use_idf=False, tfidf__smooth_idf=True, tfidf__norm=l1, clf__selected_model=('multi_nb', {'alpha': 0.25}), score=0.8378378378378378, total=  16.8s\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True, 'max_df': 1}), tfidf__use_idf=False, tfidf__smooth_idf=False, tfidf__norm=l2, clf__selected_model=('multi_nb', {'alpha': 0.1}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True, 'max_df': 1}), tfidf__use_idf=False, tfidf__smooth_idf=False, tfidf__norm=l2, clf__selected_model=('multi_nb', {'alpha': 0.1}), score=0.8320839580209896, total=  12.0s\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True, 'max_df': 1}), tfidf__use_idf=False, tfidf__smooth_idf=False, tfidf__norm=l2, clf__selected_model=('multi_nb', {'alpha': 0.1}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True, 'max_df': 1}), tfidf__use_idf=False, tfidf__smooth_idf=False, tfidf__norm=l2, clf__selected_model=('multi_nb', {'alpha': 0.1}), score=0.8312078019504876, total=  12.5s\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True, 'max_df': 1}), tfidf__use_idf=False, tfidf__smooth_idf=False, tfidf__norm=l2, clf__selected_model=('multi_nb', {'alpha': 0.1}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': True, 'max_df': 1}), tfidf__use_idf=False, tfidf__smooth_idf=False, tfidf__norm=l2, clf__selected_model=('multi_nb', {'alpha': 0.1}), score=0.8153153153153153, total=  12.6s\n",
      "[CV] vect__selected_model=('lemmatizer', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': False, 'max_df': 1}), tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l2, clf__selected_model=('multi_nb', {'alpha': 0.1}) \n",
      "[CV]  vect__selected_model=('lemmatizer', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': False, 'max_df': 1}), tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l2, clf__selected_model=('multi_nb', {'alpha': 0.1}), score=0.8403298350824587, total=  54.8s\n",
      "[CV] vect__selected_model=('lemmatizer', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': False, 'max_df': 1}), tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l2, clf__selected_model=('multi_nb', {'alpha': 0.1}) \n",
      "[CV]  vect__selected_model=('lemmatizer', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': False, 'max_df': 1}), tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l2, clf__selected_model=('multi_nb', {'alpha': 0.1}), score=0.8574643660915229, total=  55.9s\n",
      "[CV] vect__selected_model=('lemmatizer', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': False, 'max_df': 1}), tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l2, clf__selected_model=('multi_nb', {'alpha': 0.1}) \n",
      "[CV]  vect__selected_model=('lemmatizer', {'ngram_range': (1, 2), 'stop_words': None, 'lowercase': False, 'max_df': 1}), tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l2, clf__selected_model=('multi_nb', {'alpha': 0.1}), score=0.8355855855855856, total=  55.5s\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 1), 'stop_words': None, 'lowercase': False, 'max_df': 0.75}), tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l2, clf__selected_model=('multi_nb', {'alpha': 0.25}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 1), 'stop_words': None, 'lowercase': False, 'max_df': 0.75}), tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l2, clf__selected_model=('multi_nb', {'alpha': 0.25}), score=0.8620689655172413, total=   4.5s\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 1), 'stop_words': None, 'lowercase': False, 'max_df': 0.75}), tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l2, clf__selected_model=('multi_nb', {'alpha': 0.25}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 1), 'stop_words': None, 'lowercase': False, 'max_df': 0.75}), tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l2, clf__selected_model=('multi_nb', {'alpha': 0.25}), score=0.8724681170292573, total=   4.4s\n",
      "[CV] vect__selected_model=('counter', {'ngram_range': (1, 1), 'stop_words': None, 'lowercase': False, 'max_df': 0.75}), tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l2, clf__selected_model=('multi_nb', {'alpha': 0.25}) \n",
      "[CV]  vect__selected_model=('counter', {'ngram_range': (1, 1), 'stop_words': None, 'lowercase': False, 'max_df': 0.75}), tfidf__use_idf=True, tfidf__smooth_idf=False, tfidf__norm=l2, clf__selected_model=('multi_nb', {'alpha': 0.25}), score=0.8506006006006006, total=   4.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:  8.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8617154288572143"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define random search grid with cv\n",
    "rscv_clf = RandomizedSearchCV(estimator=pipeline_2, verbose=3,\n",
    "                              param_distributions=parameters_2,\n",
    "                              n_jobs=1, n_iter=5, cv=3, \n",
    "                              random_state=42)\n",
    "\n",
    "# fit model based\n",
    "rscv_clf_mod = rscv_clf.fit(x, y)\n",
    "\n",
    "# get best score from CV\n",
    "rscv_clf_mod.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vect__selected_model': ('counter',\n",
       "  {'ngram_range': (1, 1),\n",
       "   'stop_words': None,\n",
       "   'lowercase': False,\n",
       "   'max_df': 0.75}),\n",
       " 'tfidf__use_idf': True,\n",
       " 'tfidf__smooth_idf': False,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'clf__selected_model': ('multi_nb', {'alpha': 0.25})}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get parameters for best score from CV\n",
    "rscv_clf_mod.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "#rscv_clf_pred = rscv_clf_mod.best_estimator_.predict(x_validation)\n",
    "\n",
    "# model evaluation\n",
    "#print(metrics.classification_report(y_validation, rscv_clf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score Log\n",
    "\n",
    "- SGD / multi_nb randomsearchgrid with tf-idf (cv=3, n-iter=20):\n",
    "- vect__selected_model=('porter_stemmer', {'ngram_range': (1, 4), 'stop_words': None, 'lowercase': True}), tf-idf__use_idf=False, clf__selected_model=('sgd', {'alpha': 0.0001, 'loss': 'hinge', 'l1_ratio': 0.75}), score=0.9090056285178236, total=11.9min\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "After trying a handful of different bag-of-word approaches and model paramters we can conclude that the two following models performed best, both far better at either fake or real news clasification. Hopefully this can set us up for a imporved ensamble model in the last section.\n",
    "\n",
    "1. Fake News classifier:\n",
    "    -  multi_nb (Fake 96% / Real 90% acc)\n",
    "<br>\n",
    "2. Real New classifier:\n",
    "    - SGD (Real 95% / Fake 87% acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
