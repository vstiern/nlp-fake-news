{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Based Models\n",
    "**Authors**: Vilhelm Stiernstedt, Sharon Marín Salazar & Andrea Tondella\n",
    "<br>\n",
    "**Date**: 26/05/2018\n",
    "\n",
    "In this section we will conver more advanced modelling including other approaches to text processing. We wil try to use embeddning with word2vec. The main focus will be to try out different nueral networks rather than trying to acheieve the optimal accuracy. Hopefully one our models will be sufficently good and thus we can use this for our ensamble/stacking in the last section. Models to be covered:\n",
    "1. Simple Sequential\n",
    "    - tokenizer\n",
    "    - count_vectorizer\n",
    "    - tf-idf_vectorizer\n",
    "2. CNN\n",
    "3. LSTM\n",
    "    - Using Embedding\n",
    "        - simple LSTM\n",
    "        - LSTM w. CNN\n",
    "    - word2vec w. gloVe 50b dataset\n",
    "    \n",
    "#### Text Processing \n",
    "Rather than importing our cleaned data from the first section we will import the raw data and dismiss all misalinged data. Further, we will combine title and text to allow our networks to make its own conclusion with the most available information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "# general libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import urllib\n",
    "from urllib.request import urlretrieve\n",
    "from os import mkdir, makedirs, remove, listdir\n",
    "from collections import Counter\n",
    "\n",
    "# keras libs\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding, Reshape, BatchNormalization\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "# sklearn libs\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics as skMetric\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path\n",
    "wd = '/Users/Mille/GDriveMBD/Term3/NLP/Assignment_1/git-nlp-fake-news/'\n",
    "\n",
    "# import data\n",
    "df_train = pd.read_csv(wd + '/data/fake_or_real_news_training.csv')\n",
    "df_test = pd.read_csv(wd + '/data/fake_or_real_news_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              title  \\\n",
       "0   8476                       You Can Smell Hillary’s Fear   \n",
       "1  10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2   3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3  10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4    875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label   X1   X2  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  NaN  NaN  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  NaN  NaN  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  NaN  NaN  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  NaN  NaN  \n",
       "4  It's primary day in New York and front-runners...  REAL  NaN  NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect df_train\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where X1 & X2 != NaN\n",
    "df_train = df_train[(df_train.X1.isnull()) & (df_train.X2.isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop useless features -> X1 & X2\n",
    "df_train.drop(['X1', 'X2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              title  \\\n",
       "0   8476                       You Can Smell Hillary’s Fear   \n",
       "1  10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2   3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3  10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4    875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect df_train\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/pandas/core/frame.py:6201: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "# combine data\n",
    "df_all = df_train.append(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features Engineering\n",
    "- Concatenate title and text.\n",
    "\n",
    "#### Text Cleaning -> Not used ATM\n",
    "Clean title and text by preforming the following operations:\n",
    "1. Convert to lower case\n",
    "2. Remove Urls\n",
    "3. Remove everything but characters and punctuation\n",
    "4. Replace mulitple periods with a single one\n",
    "5. Replace periods with a single one\n",
    "6. Replace multple white space with a single one\n",
    "\n",
    "#### Preprocessing\n",
    "1. Split data to back to test and train\n",
    "2. Define text and label\n",
    "3. LabelEncoder to transform labels to integers:\n",
    "    - FAKE = 0\n",
    "    - REAL = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge title and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge text and title\n",
    "df_all['combined'] = df_all.title + ' ' + df_all.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    You Can Smell Hillary’s Fear Daniel Greenfield...\n",
       "1    Watch The Exact Moment Paul Ryan Committed Pol...\n",
       "2    Kerry to go to Paris in gesture of sympathy U....\n",
       "3    Bernie supporters on Twitter erupt in anger ag...\n",
       "4    The Battle of New York: Why This Primary Matte...\n",
       "Name: combined, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view \n",
    "df_all.combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text & title for df_all ->\n",
    "#df_all.title = df_all.title.str.lower() # lower case\n",
    "#df_all.text = df_all.text.str.lower() # lower case\n",
    "#df_all.title = df_all.title.str.replace(r'http[\\w:/\\.]+','<URL>') # remove urls\n",
    "#df_all.text = df_all.text.str.replace(r'http[\\w:/\\.]+','<URL>') # remove urls\n",
    "#df_all.title = df_all.title.str.replace(r'[^\\.\\w\\s]','') # remove everything but characters and punctuation\n",
    "#df_all.text = df_all.text.str.replace(r'[^\\.\\w\\s]','') # remove everything but characters and punctuation\n",
    "#df_all.title = df_all.title.str.replace(r'\\.\\.+','.') # replace multple periods with a single one\n",
    "#df_all.text = df_all.text.str.replace(r'\\.\\.+','.') # replace multiple periods with a single one\n",
    "#df_all.title = df_all.title.str.replace(r'\\.',' . ') # replace periods with a single one\n",
    "#df_all.text = df_all.text.str.replace(r'\\.',' . ') # replace multiple periods with a single one\n",
    "#df_all.title = df_all.title.str.replace(r'\\s\\s+',' ') # replace multiple white space with a single one\n",
    "#df_all.text = df_all.text.str.replace(r'\\s\\s+',' ') # replace multiple white space with a single one\n",
    "#df_all.title = df_all.title.str.strip() # strip white space\n",
    "#df_all.text = df_all.text.str.strip() # strip white space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>You Can Smell Hillary’s Fear Daniel Greenfield...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>REAL</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy U....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>REAL</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>The Battle of New York: Why This Primary Matte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID label                                               text  \\\n",
       "0   8476  FAKE  Daniel Greenfield, a Shillman Journalism Fello...   \n",
       "1  10294  FAKE  Google Pinterest Digg Linkedin Reddit Stumbleu...   \n",
       "2   3608  REAL  U.S. Secretary of State John F. Kerry said Mon...   \n",
       "3  10142  FAKE  — Kaydee King (@KaydeeKing) November 9, 2016 T...   \n",
       "4    875  REAL  It's primary day in New York and front-runners...   \n",
       "\n",
       "                                               title  \\\n",
       "0                       You Can Smell Hillary’s Fear   \n",
       "1  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        Kerry to go to Paris in gesture of sympathy   \n",
       "3  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                            combined  \n",
       "0  You Can Smell Hillary’s Fear Daniel Greenfield...  \n",
       "1  Watch The Exact Moment Paul Ryan Committed Pol...  \n",
       "2  Kerry to go to Paris in gesture of sympathy U....  \n",
       "3  Bernie supporters on Twitter erupt in anger ag...  \n",
       "4  The Battle of New York: Why This Primary Matte...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view changes\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data again\n",
    "train = df_all[:len(df_train)]\n",
    "test = df_all[len(df_train):]\n",
    "\n",
    "# subset text & label\n",
    "text = train.combined\n",
    "label = train.label\n",
    "\n",
    "# define labelencoder and preprocess label\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing for Simple Sequential Model\n",
    "- Bag of Words - Vectorizing texts trying:\n",
    "    - Tokenizer (count, freq, tf-idf)\n",
    "    - CountVectorier\n",
    "    - tf-idf (Sklearn)\n",
    "    - one-hot encoding (combined text and title) -> not working atm\n",
    "- Parameters to try:\n",
    "    - ngrams\n",
    "    - max_features / num_words (words used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 1]\n",
      "[[ 0. 93. 34. ...  0.  0.  0.]\n",
      " [ 0. 16. 14. ...  0.  0.  0.]\n",
      " [ 0. 24. 15. ...  0.  0.  0.]\n",
      " [ 0. 25. 14. ...  0.  0.  0.]\n",
      " [ 0. 19. 13. ...  0.  0.  0.]]\n",
      "(3966,) (3966, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Mehtod 1 \n",
    "# define maxium features -> arbitary\n",
    "num_max = 5000\n",
    "\n",
    "# define tokenziner\n",
    "tok = Tokenizer(num_words=num_max, lower=True)\n",
    "# tokenize\n",
    "tok.fit_on_texts(text)\n",
    "# encode document (alter between mode; count, tfidf)\n",
    "mat_text = tok.texts_to_matrix(text, mode='count')\n",
    "\n",
    "# preview process\n",
    "print(label[:5])\n",
    "print(mat_text[:5])\n",
    "print(label.shape, mat_text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 1]\n",
      "[[ 0. 93. 34. ...  0.  0.  0.]\n",
      " [ 0. 17. 14. ...  0.  0.  0.]\n",
      " [ 0. 24. 17. ...  0.  0.  0.]\n",
      " [ 0. 26. 15. ...  0.  0.  0.]\n",
      " [ 0. 20. 13. ...  0.  0.  0.]]\n",
      "(3966,) (3966, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Method 2 \n",
    "# define maxium features\n",
    "num_max = 5000\n",
    "\n",
    "# define count vectorizer\n",
    "count_vectorizer = CountVectorizer(max_features=num_max) #, ngram_range=(1,3)) # parameters to change ngrams\n",
    "# tokenize \n",
    "count_vectorizer.fit(text)\n",
    "# encode document\n",
    "count_vector = count_vectorizer.transform(text)\n",
    "\n",
    "# preview process\n",
    "print(label[:5])\n",
    "print(mat_text[:5])\n",
    "print(label.shape, count_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 1]\n",
      "[[ 0. 93. 34. ...  0.  0.  0.]\n",
      " [ 0. 17. 14. ...  0.  0.  0.]\n",
      " [ 0. 24. 17. ...  0.  0.  0.]\n",
      " [ 0. 26. 15. ...  0.  0.  0.]\n",
      " [ 0. 20. 13. ...  0.  0.  0.]]\n",
      "(3966,) (3966, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Method 3\n",
    "# define maxium features\n",
    "num_max = 5000\n",
    "\n",
    "# tf-idf (Term Frequency times inverse document frequency)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=num_max) #, ngram_range=(1,3)) # parameters to change ngrams\n",
    "# tokenize and build vocab \n",
    "tfidf_vectorizer.fit(text)\n",
    "# encode document\n",
    "tfidf_vector = tfidf_vectorizer.transform(text)\n",
    "\n",
    "# preview process\n",
    "print(label[:5])\n",
    "print(mat_text[:5])\n",
    "print(label.shape, tfidf_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 4 -> one-hot encoding \n",
    "# text_to_word_sequence doesn't allow for pd series input. no solution find as of now. method put on hold.\n",
    "# import libs\n",
    "#from keras.preprocessing.text import one_hot\n",
    "#from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# define maxium features\n",
    "#num_max = 5000\n",
    "\n",
    "# estimate the size of the vocabulary\n",
    "#words = text_to_word_sequence(text, split=' ', lower=False)\n",
    "#vocab_size = len(words)\n",
    "\n",
    "# integer encode the document\n",
    "#onehot_encode = one_hot(text, round(vocab_size*1.3))\n",
    "\n",
    "# preview process\n",
    "#print(label[:5])\n",
    "#print(one_hot_text[:5])\n",
    "#print(label.shape, one_hot_text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split -> training / validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training (80%) and test/validation (20%)\n",
    "x_train, x_test, y_train, y_test = train_test_split(mat_text, label,\n",
    "                                                    test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential Model \n",
    "1. Define Sequential model function:\n",
    "    - First layer 512 neurons with an 'relu' activation\n",
    "    - Second layer 256 neurons with an 'relu' activation\n",
    "    - Loss function: 'binary_crossentropy'\n",
    "    - Metric: Accuracy\n",
    "<br>\n",
    "<br>\n",
    "2. Define model pipeline that holds range of model inputs:\n",
    "    - batch size \n",
    "    - optimizers\n",
    "    - epochs\n",
    "    - kernel intitializers\n",
    "3. Use random serach grid to test all paramters\n",
    "4. Evaluate model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sequential model\n",
    "def create_simple_model(optimizer='adam', init='glorot_uniform', dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    # first layer\n",
    "    model.add(Dense(512,\n",
    "                    activation='relu',\n",
    "                    input_shape=(num_max,),\n",
    "                    kernel_initializer=init)\n",
    "             ) \n",
    "    model.add(Dropout(dropout_rate)) # dropout\n",
    "    # second layer\n",
    "    model.add(Dense(256,\n",
    "                    activation='relu'\n",
    "                    ,kernel_initializer=init)\n",
    "             )\n",
    "    model.add(Dropout(dropout_rate)) # dropout \n",
    "    # out_layer\n",
    "    model.add(Dense(1,\n",
    "                    activation='sigmoid',\n",
    "                    kernel_initializer=init)\n",
    "             )\n",
    "    # summary\n",
    "    model.summary()\n",
    "    # compiler\n",
    "    model.compile(loss='binary_crossentropy',  # loss function\n",
    "              optimizer=optimizer, # optimizer\n",
    "              metrics=['acc', metrics.abinary_accuracy]) # metric \n",
    "    print('compile done')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 512)               2560512   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 2,692,097\n",
      "Trainable params: 2,692,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "compile done\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 512)               2560512   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 2,692,097\n",
      "Trainable params: 2,692,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "compile done\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 512)               2560512   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 2,692,097\n",
      "Trainable params: 2,692,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "compile done\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 512)               2560512   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 2,692,097\n",
      "Trainable params: 2,692,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "compile done\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 512)               2560512   \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 2,692,097\n",
      "Trainable params: 2,692,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "compile done\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 512)               2560512   \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 2,692,097\n",
      "Trainable params: 2,692,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "compile done\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 512)               2560512   \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 2,692,097\n",
      "Trainable params: 2,692,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "compile done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x1125441d0>,\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'optimizer': ['adam'], 'epochs': [20], 'batch_size': [32], 'init': ['glorot_uniform'], 'dropout_rate': [0.1, 0.5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "seq_m = KerasClassifier(build_fn=create_simple_model,\n",
    "                        verbose=0)\n",
    "\n",
    "# specify random grid search paramters \n",
    "optimizers = ['adam']\n",
    "epochs = [5]\n",
    "batch_size = [32]\n",
    "init = ['glorot_uniform'] # , 'normal', 'uniform']\n",
    "dropout_rate = [0.75] \n",
    "\n",
    "# add paramters into dict\n",
    "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batch_size,\n",
    "                  init=init, dropout_rate=dropout_rate)\n",
    "\n",
    "# specify randsearch gird\n",
    "seq_m_grid = GridSearchCV(estimator=seq_m, param_grid=param_grid,\n",
    "                    n_jobs=1, cv=3)\n",
    "\n",
    "# fit model pipelineg\n",
    "seq_m_grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.913619 using {'batch_size': 32, 'dropout_rate': 0.5, 'epochs': 20, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.902900 (0.014312) with: {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
      "0.913619 (0.003832) with: {'batch_size': 32, 'dropout_rate': 0.5, 'epochs': 20, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "# summarize results from model output -> highest score and associated parameters\n",
    "print(\"Best: %f using %s\" % (seq_m_grid.best_score_, seq_m_grid.best_params_))\n",
    "\n",
    "# specify results for output\n",
    "means = seq_m_grid.cv_results_['mean_test_score']\n",
    "stds = seq_m_grid.cv_results_['std_test_score']\n",
    "params = seq_m_grid.cv_results_['params']\n",
    "\n",
    "# loop to print each element for each model result\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "seq_m_grid_pred = seq_m_grid.best_estimator_.predict(x_test)\n",
    "seq_m_grid_pred = (seq_m_grid_pred > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.954     0.896     0.924       396\n",
      "          1      0.903     0.957     0.929       398\n",
      "\n",
      "avg / total      0.929     0.927     0.927       794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model evaluation\n",
    "print(skMetric.classification_report(y_test, seq_m_grid_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Score Summary\n",
    "#### GridSearch Approach (CV=3)\n",
    "1. Best: 0.928643 using {'batch_size': 64, 'epochs': 10, 'optimizer': 'rmsprop'}\n",
    "2. Best: 0.928643 using {'batch_size': 32, 'dropout_rate': 0.1, 'epochs': 20, 'init': 'glorot_uniform', 'optimizer': 'adam'} -> using combined title and text.\n",
    "\n",
    "#### Single Model Approach \n",
    "1. val_binary_accuracy: 93.87%\n",
    "    - using keras tokenziner\n",
    "    - convert to lower case\n",
    "    - num_max = 5000\n",
    "    - Dropout: 0.1 (maybe overfitting?)\n",
    "    - batch size: 32\n",
    "    - epochs: 20\n",
    "    - validation: 0.2\n",
    "    - First layer 512 neurons with an 'relu' activation\n",
    "    - Second layer 256 neurons with an 'relu' activation\n",
    "    - Loss function: 'binary_crossentropy'\n",
    "    - Metric: Accuracy\n",
    "2. val_binary_accuracy: 91.37%\n",
    "    - using count_vectorizer\n",
    "    - convert to lower case\n",
    "    - num_max = 5000\n",
    "    - Dropout: 0.1 (maybe overfitting?)\n",
    "    - batch size: 32\n",
    "    - epochs: 20\n",
    "    - validation: 0.2\n",
    "    - First layer 512 neurons with an 'relu' activation\n",
    "    - Second layer 256 neurons with an 'relu' activation\n",
    "    - Loss function: 'binary_crossentropy'\n",
    "    - Metric: Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model \n",
    "Next we will try to use a CNN. Our model is based on the following design:\n",
    "1. Sequenicing\n",
    "    - max length of sequance\n",
    "1. Define Sequential model function:\n",
    "    - First convolution layer 64 neurons with an 'relu' activation\n",
    "    - Second convolution layer 25 neurons with an 'relu' activation\n",
    "    - Dropout: 0.2 (value to prevent overfitting)\n",
    "    - Loss function: 'binary_crossentropy'\n",
    "    - Metric: Accuracy\n",
    "<br>\n",
    "<br>\n",
    "2. Define model fitting function:\n",
    "    - batch size: 32\n",
    "    - epochs: 10 (arbitary cut-off point, 10 is analogous to ten fold cross validation of data)\n",
    "    - validation split: 20% (training data 80%, validation 20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function that defines CNN model\n",
    "def get_cnn_model():   \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_max, 20, input_length=max_len))\n",
    "    model.add(Dropout(0.75))\n",
    "    # first layer\n",
    "    model.add(Conv1D(filters=64, \n",
    "                     kernel_size=5,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    # second layer\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(Activation('relu'))\n",
    "    # out layer\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc', metrics.binary_accuracy])\n",
    "    return model\n",
    "\n",
    "# define function for model fitting\n",
    "def check_cnn_model(model, x , y):\n",
    "    model.fit(x, y, \n",
    "              batch_size=128,\n",
    "              epochs=40, \n",
    "              verbose=1,\n",
    "              validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenzining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 1]\n",
      "[[ 0. 74. 34. ...  0.  0.  0.]\n",
      " [ 0. 13. 14. ...  0.  0.  0.]\n",
      " [ 0. 20. 17. ...  0.  0.  0.]\n",
      " [ 0. 21. 15. ...  0.  0.  0.]\n",
      " [ 0. 19. 13. ...  0.  0.  0.]]\n",
      "(3966,) (3966, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Mehtod 1 \n",
    "# define tokenziner\n",
    "tok = Tokenizer(num_words=num_max, lower=False)\n",
    "# tokenize\n",
    "tok.fit_on_texts(text)\n",
    "# encode document (alter between mode; count, tfidf)\n",
    "mat_text = tok.texts_to_matrix(text, mode='count')\n",
    "\n",
    "# preview process\n",
    "print(label[:5])\n",
    "print(mat_text[:5])\n",
    "print(label.shape, mat_text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3966, 250)\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing for cnn\n",
    "# define max_len for sequence\n",
    "max_len = 300\n",
    "# sequence tokinzed text\n",
    "cnn_texts_seq = tok.texts_to_sequences(text)\n",
    "# ensure that all sequences have the same length\n",
    "cnn_texts_mat = sequence.pad_sequences(cnn_texts_seq, maxlen=max_len)\n",
    "\n",
    "# view process\n",
    "#print(cnn_texts_seq[0])\n",
    "#print(cnn_texts_mat[0])\n",
    "print(cnn_texts_mat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 250, 20)           100000    \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 250, 20)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 246, 64)           6464      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_10 (Glo (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 123,361\n",
      "Trainable params: 123,361\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3172 samples, validate on 794 samples\n",
      "Epoch 1/40\n",
      "3172/3172 [==============================] - 4s 1ms/step - loss: 0.6959 - acc: 0.4991 - binary_accuracy: 0.4991 - val_loss: 0.6941 - val_acc: 0.4723 - val_binary_accuracy: 0.4723\n",
      "Epoch 2/40\n",
      "3172/3172 [==============================] - 3s 846us/step - loss: 0.6953 - acc: 0.4940 - binary_accuracy: 0.4940 - val_loss: 0.6930 - val_acc: 0.4736 - val_binary_accuracy: 0.4736\n",
      "Epoch 3/40\n",
      "3172/3172 [==============================] - 3s 860us/step - loss: 0.6935 - acc: 0.5079 - binary_accuracy: 0.5079 - val_loss: 0.6923 - val_acc: 0.4987 - val_binary_accuracy: 0.4987\n",
      "Epoch 4/40\n",
      "3172/3172 [==============================] - 3s 907us/step - loss: 0.6913 - acc: 0.5271 - binary_accuracy: 0.5271 - val_loss: 0.6912 - val_acc: 0.4723 - val_binary_accuracy: 0.4723\n",
      "Epoch 5/40\n",
      "3172/3172 [==============================] - 4s 1ms/step - loss: 0.6808 - acc: 0.5760 - binary_accuracy: 0.5760 - val_loss: 0.6752 - val_acc: 0.6889 - val_binary_accuracy: 0.6889\n",
      "Epoch 6/40\n",
      "3172/3172 [==============================] - 3s 976us/step - loss: 0.6503 - acc: 0.6384 - binary_accuracy: 0.6384 - val_loss: 0.6459 - val_acc: 0.6738 - val_binary_accuracy: 0.6738\n",
      "Epoch 7/40\n",
      "3172/3172 [==============================] - 3s 1ms/step - loss: 0.6097 - acc: 0.7235 - binary_accuracy: 0.7235 - val_loss: 0.6123 - val_acc: 0.7015 - val_binary_accuracy: 0.7015\n",
      "Epoch 8/40\n",
      "3172/3172 [==============================] - 3s 998us/step - loss: 0.5553 - acc: 0.7418 - binary_accuracy: 0.7418 - val_loss: 0.5530 - val_acc: 0.7582 - val_binary_accuracy: 0.7582\n",
      "Epoch 9/40\n",
      "3172/3172 [==============================] - 4s 1ms/step - loss: 0.4571 - acc: 0.7973 - binary_accuracy: 0.7973 - val_loss: 0.4589 - val_acc: 0.7834 - val_binary_accuracy: 0.7834\n",
      "Epoch 10/40\n",
      "3172/3172 [==============================] - 4s 1ms/step - loss: 0.3672 - acc: 0.8468 - binary_accuracy: 0.8468 - val_loss: 0.3845 - val_acc: 0.8212 - val_binary_accuracy: 0.8212\n",
      "Epoch 11/40\n",
      "3172/3172 [==============================] - 3s 966us/step - loss: 0.3022 - acc: 0.8783 - binary_accuracy: 0.8783 - val_loss: 0.3216 - val_acc: 0.8640 - val_binary_accuracy: 0.8640\n",
      "Epoch 12/40\n",
      "3172/3172 [==============================] - 3s 882us/step - loss: 0.2714 - acc: 0.8900 - binary_accuracy: 0.8900 - val_loss: 0.3496 - val_acc: 0.8401 - val_binary_accuracy: 0.8401\n",
      "Epoch 13/40\n",
      "3172/3172 [==============================] - 3s 962us/step - loss: 0.2428 - acc: 0.9086 - binary_accuracy: 0.9086 - val_loss: 0.3357 - val_acc: 0.8463 - val_binary_accuracy: 0.8463\n",
      "Epoch 14/40\n",
      "3172/3172 [==============================] - 3s 859us/step - loss: 0.2113 - acc: 0.9171 - binary_accuracy: 0.9171 - val_loss: 0.3218 - val_acc: 0.8501 - val_binary_accuracy: 0.8501\n",
      "Epoch 15/40\n",
      "3172/3172 [==============================] - 3s 824us/step - loss: 0.1956 - acc: 0.9243 - binary_accuracy: 0.9243 - val_loss: 0.3054 - val_acc: 0.8577 - val_binary_accuracy: 0.8577\n",
      "Epoch 16/40\n",
      "3172/3172 [==============================] - 3s 893us/step - loss: 0.1751 - acc: 0.9351 - binary_accuracy: 0.9351 - val_loss: 0.3018 - val_acc: 0.8627 - val_binary_accuracy: 0.8627\n",
      "Epoch 17/40\n",
      "3172/3172 [==============================] - 3s 813us/step - loss: 0.1677 - acc: 0.9376 - binary_accuracy: 0.9376 - val_loss: 0.3073 - val_acc: 0.8640 - val_binary_accuracy: 0.8640\n",
      "Epoch 18/40\n",
      "3172/3172 [==============================] - 3s 829us/step - loss: 0.1666 - acc: 0.9445 - binary_accuracy: 0.9445 - val_loss: 0.3008 - val_acc: 0.8690 - val_binary_accuracy: 0.8690\n",
      "Epoch 19/40\n",
      "3172/3172 [==============================] - 3s 832us/step - loss: 0.1444 - acc: 0.9492 - binary_accuracy: 0.9492 - val_loss: 0.2956 - val_acc: 0.8728 - val_binary_accuracy: 0.8728\n",
      "Epoch 20/40\n",
      "3172/3172 [==============================] - 3s 970us/step - loss: 0.1269 - acc: 0.9537 - binary_accuracy: 0.9537 - val_loss: 0.3447 - val_acc: 0.8438 - val_binary_accuracy: 0.8438\n",
      "Epoch 21/40\n",
      "3172/3172 [==============================] - 3s 961us/step - loss: 0.1107 - acc: 0.9596 - binary_accuracy: 0.9596 - val_loss: 0.2958 - val_acc: 0.8778 - val_binary_accuracy: 0.8778\n",
      "Epoch 22/40\n",
      "3172/3172 [==============================] - 3s 1ms/step - loss: 0.1127 - acc: 0.9606 - binary_accuracy: 0.9606 - val_loss: 0.3183 - val_acc: 0.8652 - val_binary_accuracy: 0.8652\n",
      "Epoch 23/40\n",
      "3172/3172 [==============================] - 3s 899us/step - loss: 0.0998 - acc: 0.9666 - binary_accuracy: 0.9666 - val_loss: 0.3033 - val_acc: 0.8728 - val_binary_accuracy: 0.8728\n",
      "Epoch 24/40\n",
      "3172/3172 [==============================] - 3s 934us/step - loss: 0.0988 - acc: 0.9653 - binary_accuracy: 0.9653 - val_loss: 0.2822 - val_acc: 0.8854 - val_binary_accuracy: 0.8854\n",
      "Epoch 25/40\n",
      "3172/3172 [==============================] - 3s 910us/step - loss: 0.0979 - acc: 0.9678 - binary_accuracy: 0.9678 - val_loss: 0.3099 - val_acc: 0.8678 - val_binary_accuracy: 0.8678\n",
      "Epoch 26/40\n",
      "3172/3172 [==============================] - 3s 1ms/step - loss: 0.0986 - acc: 0.9666 - binary_accuracy: 0.9666 - val_loss: 0.3245 - val_acc: 0.8615 - val_binary_accuracy: 0.8615\n",
      "Epoch 27/40\n",
      "3172/3172 [==============================] - 3s 1ms/step - loss: 0.0818 - acc: 0.9719 - binary_accuracy: 0.9719 - val_loss: 0.3173 - val_acc: 0.8715 - val_binary_accuracy: 0.8715\n",
      "Epoch 28/40\n",
      "3172/3172 [==============================] - 3s 933us/step - loss: 0.0835 - acc: 0.9697 - binary_accuracy: 0.9697 - val_loss: 0.3161 - val_acc: 0.8728 - val_binary_accuracy: 0.8728\n",
      "Epoch 29/40\n",
      "3172/3172 [==============================] - 3s 1ms/step - loss: 0.0799 - acc: 0.9738 - binary_accuracy: 0.9738 - val_loss: 0.3083 - val_acc: 0.8753 - val_binary_accuracy: 0.8753\n",
      "Epoch 30/40\n",
      "3172/3172 [==============================] - 3s 1ms/step - loss: 0.0658 - acc: 0.9779 - binary_accuracy: 0.9779 - val_loss: 0.2965 - val_acc: 0.8917 - val_binary_accuracy: 0.8917\n",
      "Epoch 31/40\n",
      "3172/3172 [==============================] - 3s 1ms/step - loss: 0.0772 - acc: 0.9713 - binary_accuracy: 0.9713 - val_loss: 0.3032 - val_acc: 0.8854 - val_binary_accuracy: 0.8854\n",
      "Epoch 32/40\n",
      "3172/3172 [==============================] - 4s 1ms/step - loss: 0.0808 - acc: 0.9751 - binary_accuracy: 0.9751 - val_loss: 0.3268 - val_acc: 0.8652 - val_binary_accuracy: 0.8652\n",
      "Epoch 33/40\n",
      "3172/3172 [==============================] - 3s 1ms/step - loss: 0.0706 - acc: 0.9760 - binary_accuracy: 0.9760 - val_loss: 0.3436 - val_acc: 0.8640 - val_binary_accuracy: 0.8640\n",
      "Epoch 34/40\n",
      "3172/3172 [==============================] - 3s 984us/step - loss: 0.0647 - acc: 0.9757 - binary_accuracy: 0.9757 - val_loss: 0.3535 - val_acc: 0.8640 - val_binary_accuracy: 0.8640\n",
      "Epoch 35/40\n",
      "3172/3172 [==============================] - 3s 906us/step - loss: 0.0585 - acc: 0.9798 - binary_accuracy: 0.9798 - val_loss: 0.3382 - val_acc: 0.8728 - val_binary_accuracy: 0.8728\n",
      "Epoch 36/40\n",
      "3172/3172 [==============================] - 3s 998us/step - loss: 0.0568 - acc: 0.9808 - binary_accuracy: 0.9808 - val_loss: 0.3235 - val_acc: 0.8791 - val_binary_accuracy: 0.8791\n",
      "Epoch 37/40\n",
      "3172/3172 [==============================] - 3s 878us/step - loss: 0.0630 - acc: 0.9767 - binary_accuracy: 0.9767 - val_loss: 0.3372 - val_acc: 0.8766 - val_binary_accuracy: 0.8766\n",
      "Epoch 38/40\n",
      "3172/3172 [==============================] - 3s 869us/step - loss: 0.0607 - acc: 0.9773 - binary_accuracy: 0.9773 - val_loss: 0.3602 - val_acc: 0.8728 - val_binary_accuracy: 0.8728\n",
      "Epoch 39/40\n",
      "3172/3172 [==============================] - 3s 996us/step - loss: 0.0613 - acc: 0.9786 - binary_accuracy: 0.9786 - val_loss: 0.3289 - val_acc: 0.8728 - val_binary_accuracy: 0.8728\n",
      "Epoch 40/40\n",
      "3172/3172 [==============================] - 3s 988us/step - loss: 0.0501 - acc: 0.9836 - binary_accuracy: 0.9836 - val_loss: 0.3234 - val_acc: 0.8829 - val_binary_accuracy: 0.8829\n"
     ]
    }
   ],
   "source": [
    "# assign model\n",
    "cnn_m = get_cnn_model()\n",
    "\n",
    "# fit model with predifined and processed text and label\n",
    "check_cnn_model(cnn_m, cnn_texts_mat, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evalulation\n",
    "1. val_binary_accuracy: 0.9063:\n",
    "    - max_len = 300\n",
    "    - dropout = 0.5\n",
    "    - lower = False\n",
    "    - combined text \n",
    "2. val_binary_accuracy: 0.8917:\n",
    "    - max_len = 200\n",
    "    - dropout = 0.5\n",
    "    - lower = False\n",
    "    - combined text \n",
    "3. val_binary_accuracy: 0.8917:\n",
    "    - max_len = 250\n",
    "    - dropout = 0.75\n",
    "    - lower = False\n",
    "    - combined text \n",
    "    - kernel_size=5\n",
    "    - batch_size = 128\n",
    "    - epochs = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model w. Embedding\n",
    "Next we will try a LSTM model together with word embeddnings using GLOVE vector dataset. Word Embedding is the representation of text in the form of vectors. The underlying idea here is that similar words will have a minimum distance between their vectors.\n",
    "\n",
    "1. Import Glove dataset (using 50d)\n",
    "2. tokennize\n",
    "3. embedding\n",
    "4. model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import glove dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# define embedding dict\n",
    "embeddings_index = {}\n",
    "# import glove dataset \n",
    "glove_data = wd + 'glove.6B/glove.6B.50d.txt'\n",
    "# process glove data and insert into embed dict\n",
    "f = open(glove_data)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0] \n",
    "    value = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = value\n",
    "f.close()\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tokenziner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define max word\n",
    "max_words = 5000\n",
    "# define tokenziner\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "# tokenize text\n",
    "tokenizer.fit_on_texts(train.text)\n",
    "# encode document (alter between mode; count, tfidf)\n",
    "#lstm_text = tokenizer.texts_to_matrix(text, mode='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### embeddning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76848, 32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define embedding dimension\n",
    "embedding_dimension = 32\n",
    "# assign tokenizeer word index\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# define embedding_matrix matrix (maps words to vectors as per specified embedding dimension)\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dimension))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector[:embedding_dimension]\n",
    "\n",
    "# view matrix dimension\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define max length of sequence\n",
    "max_review_len = 300\n",
    "# define embedding layer\n",
    "embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                            embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_review_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sequencing  / padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape text to word-to-index sequences\n",
    "lstm_text = tokenizer.texts_to_sequences(train.text)\n",
    "lstm_text = pad_sequences(lstm_text, maxlen=max_review_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split -> training / validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training (80%) and test/validation (20%)\n",
    "x_train, x_test, y_train, y_test = train_test_split(lstm_text, label,\n",
    "                                                    test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 300, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 3172 samples, validate on 794 samples\n",
      "Epoch 1/5\n",
      "3172/3172 [==============================] - 23s 7ms/step - loss: 0.6813 - acc: 0.5854 - val_loss: 0.6474 - val_acc: 0.6688\n",
      "Epoch 2/5\n",
      "3172/3172 [==============================] - 22s 7ms/step - loss: 0.4941 - acc: 0.7926 - val_loss: 0.4155 - val_acc: 0.8111\n",
      "Epoch 3/5\n",
      "3172/3172 [==============================] - 20s 6ms/step - loss: 0.3254 - acc: 0.8799 - val_loss: 0.3420 - val_acc: 0.8589\n",
      "Epoch 4/5\n",
      "3172/3172 [==============================] - 20s 6ms/step - loss: 0.2457 - acc: 0.9102 - val_loss: 0.3407 - val_acc: 0.8715\n",
      "Epoch 5/5\n",
      "3172/3172 [==============================] - 23s 7ms/step - loss: 0.1615 - acc: 0.9426 - val_loss: 0.3808 - val_acc: 0.8275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x106fcba58>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model \n",
    "model = Sequential()\n",
    "# add embedding dimensions\n",
    "model.add(Embedding(input_dim=max_words,\n",
    "                    output_dim=embedding_dimension, \n",
    "                    input_length=max_review_len))\n",
    "# add LSTM layer -> units, tanh activation, dropout/r_dropout\n",
    "model.add(LSTM(units=100, dropout=0.2, recurrent_dropout=0.2))\n",
    "# add output layer\n",
    "model.add(Dense(1, \n",
    "                activation='sigmoid'))\n",
    "# define compiler\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# view model summary\n",
    "print(model.summary())\n",
    "\n",
    "# fit model\n",
    "model.fit(x=x_train, y=y_train,\n",
    "          epochs=5,\n",
    "          batch_size=64,\n",
    "          validation_data=(x_test, y_test),\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "794/794 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.38082174007177955, 0.8274559186447777]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model evaluation\n",
    "-> best score 88.15% using:\n",
    "- max_num = 5000\n",
    "- max_review_length = 300\n",
    "- embedding_dimension = 32,\n",
    "- dropout = 0.2\n",
    "- epoch = 5\n",
    "- batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM and CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_25 (Embedding)     (None, 300, 32)           160000    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 300, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 300, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 150, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 216,405\n",
      "Trainable params: 216,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 3172 samples, validate on 794 samples\n",
      "Epoch 1/5\n",
      "3172/3172 [==============================] - 13s 4ms/step - loss: 0.6757 - acc: 0.5883 - val_loss: 0.7119 - val_acc: 0.6549\n",
      "Epoch 2/5\n",
      "3172/3172 [==============================] - 11s 3ms/step - loss: 0.4656 - acc: 0.7888 - val_loss: 0.3588 - val_acc: 0.8552\n",
      "Epoch 3/5\n",
      "3172/3172 [==============================] - 10s 3ms/step - loss: 0.2524 - acc: 0.9001 - val_loss: 0.2827 - val_acc: 0.8866\n",
      "Epoch 4/5\n",
      "3172/3172 [==============================] - 11s 4ms/step - loss: 0.1594 - acc: 0.9395 - val_loss: 0.2870 - val_acc: 0.8816\n",
      "Epoch 5/5\n",
      "3172/3172 [==============================] - 12s 4ms/step - loss: 0.1027 - acc: 0.9634 - val_loss: 0.3238 - val_acc: 0.8602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x124a0bb00>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the lstm_cnn_m \n",
    "lstm_cnn_m = Sequential()\n",
    "# add embedding dimensions\n",
    "lstm_cnn_m.add(Embedding(input_dim=max_words,\n",
    "                    output_dim=embedding_dimension, \n",
    "                    input_length=max_review_len))\n",
    "# add dropout\n",
    "lstm_cnn_m.add(Dropout(0.2))\n",
    "# add convolution layer\n",
    "lstm_cnn_m.add(Conv1D(filters=embedding_dimension,\n",
    "                 kernel_size=3,\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "# add maxpooling \n",
    "lstm_cnn_m.add(MaxPooling1D(pool_size=2))\n",
    "# add dropout\n",
    "# add LSTM layer -> units, tanh activation, dropout/r_dropout\n",
    "lstm_cnn_m.add(LSTM(units=100, dropout=0.2))\n",
    "# add dropout\n",
    "# add output layer\n",
    "lstm_cnn_m.add(Dense(1, \n",
    "                activation='sigmoid'))\n",
    "# define compiler\n",
    "lstm_cnn_m.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# view lstm_cnn_m summary\n",
    "print(lstm_cnn_m.summary())\n",
    "\n",
    "# fit lstm_cnn_m\n",
    "lstm_cnn_m.fit(x=x_train, y=y_train,\n",
    "          epochs=5,\n",
    "          batch_size=64,\n",
    "          validation_data=(x_test, y_test),\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "lstm_cnn_pred = lstm_cnn_m.predict(x_test)\n",
    "lstm_cnn_pred = (lstm_cnn_pred > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0      0.882     0.831     0.856       396\n",
      "          1      0.841     0.889     0.864       398\n",
      "\n",
      "avg / total      0.861     0.860     0.860       794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model evaluation\n",
    "print(metrics.classification_report(y_test, lstm_cnn_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model w. word2vec\n",
    "Following code proposed by github user: sachinruk. Code notations are made by notebooks authers.\n",
    "\n",
    "https://github.com/sachinruk/deepschool.io/blob/master/DL-Keras_Tensorflow/Lesson%2018%20-%20Fake%20News%20Classification%20-%20Solutions.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import glove dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glove dataset \n",
    "with open(wd + 'glove.6B/glove.6B.50d.txt','rb') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# process glove data \n",
    "glove_weights = np.zeros((len(lines), 50))\n",
    "words = []\n",
    "# get weights and words from dataset\n",
    "for i, line in enumerate(lines):\n",
    "    word_weights = line.split()\n",
    "    words.append(word_weights[0])\n",
    "    weight = word_weights[1:]\n",
    "    glove_weights[i] = np.array([float(w) for w in weight])\n",
    "word_vocab = [w.decode(\"utf-8\") for w in words]\n",
    "\n",
    "# save word and weights into dictionary \n",
    "word2glove = dict(zip(word_vocab, glove_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "import keras.backend as K\n",
    "from keras import initializers\n",
    "import numpy as np\n",
    "\n",
    "class Embedding2(Layer):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, fixed_weights, embeddings_initializer='uniform', \n",
    "                 input_length=None, **kwargs):\n",
    "        kwargs['dtype'] = 'int32'\n",
    "        if 'input_shape' not in kwargs:\n",
    "            if input_length:\n",
    "                kwargs['input_shape'] = (input_length,)\n",
    "            else:\n",
    "                kwargs['input_shape'] = (None,)\n",
    "        super(Embedding2, self).__init__(**kwargs)\n",
    "    \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.embeddings_initializer = embeddings_initializer\n",
    "        self.fixed_weights = fixed_weights\n",
    "        self.num_trainable = input_dim - len(fixed_weights)\n",
    "        self.input_length = input_length\n",
    "        \n",
    "        w_mean = fixed_weights.mean(axis=0)\n",
    "        w_std = fixed_weights.std(axis=0)\n",
    "        self.variable_weights = w_mean + w_std*np.random.randn(self.num_trainable, output_dim)\n",
    "\n",
    "    def build(self, input_shape, name='embeddings'):        \n",
    "        fixed_weight = K.variable(self.fixed_weights, name=name+'_fixed')\n",
    "        variable_weight = K.variable(self.variable_weights, name=name+'_var')\n",
    "        \n",
    "        self._trainable_weights.append(variable_weight)\n",
    "        self._non_trainable_weights.append(fixed_weight)\n",
    "        \n",
    "        self.embeddings = K.concatenate([fixed_weight, variable_weight], axis=0)\n",
    "        \n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if K.dtype(inputs) != 'int32':\n",
    "            inputs = K.cast(inputs, 'int32')\n",
    "        out = K.gather(self.embeddings, inputs)\n",
    "        return out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if not self.input_length:\n",
    "            input_length = input_shape[1]\n",
    "        else:\n",
    "            input_length = self.input_length\n",
    "        return (input_shape[0], input_length, self.output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text & title for df_train ->\n",
    "df_train.title = df_train.title.str.lower() # lower case\n",
    "df_train.text = df_train.text.str.lower() # lower case\n",
    "df_train.title = df_train.title.str.replace(r'http[\\w:/\\.]+','<URL>') # remove urls\n",
    "df_train.text = df_train.text.str.replace(r'http[\\w:/\\.]+','<URL>') # remove urls\n",
    "df_train.title = df_train.title.str.replace(r'[^\\.\\w\\s]','') # remove everything but characters and punctuation\n",
    "df_train.text = df_train.text.str.replace(r'[^\\.\\w\\s]','') # remove everything but characters and punctuation\n",
    "df_train.title = df_train.title.str.replace(r'\\.\\.+','.') # replace multple periods with a single one\n",
    "df_train.text = df_train.text.str.replace(r'\\.\\.+','.') # replace multiple periods with a single one\n",
    "df_train.title = df_train.title.str.replace(r'\\.',' . ') # replace periods with a single one\n",
    "df_train.text = df_train.text.str.replace(r'\\.',' . ') # replace multiple periods with a single one\n",
    "df_train.title = df_train.title.str.replace(r'\\s\\s+',' ') # replace multiple white space with a single one\n",
    "df_train.text = df_train.text.str.replace(r'\\s\\s+',' ') # replace multiple white space with a single one\n",
    "df_train.title = df_train.title.str.strip() # strip white space\n",
    "df_train.text = df_train.text.str.strip() # strip white space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### unique words (occurrance >5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of unique words in glove vectors:  0.7195547700083261\n"
     ]
    }
   ],
   "source": [
    "# join all text\n",
    "all_text = ' '.join(df_train.text.values)\n",
    "\n",
    "# split into words\n",
    "words = all_text.split()\n",
    "\n",
    "# count most common words\n",
    "u_words = Counter(words).most_common()\n",
    "\n",
    "# save copy of most common words\n",
    "u_words_counter = u_words\n",
    "\n",
    "# subset for words used more than 5 times\n",
    "u_words_frequent = [word[0] for word in u_words if word[1]>5] \n",
    "\n",
    "# get total word count\n",
    "u_words_total = [k for k,v in u_words_counter]\n",
    "\n",
    "# merge total count into with existing glove dictionary \n",
    "word_vocab = dict(zip(word_vocab, range(len(word_vocab))))\n",
    "\n",
    "# convert into np array\n",
    "word_in_glove = np.array([w in word_vocab for w in u_words_total])\n",
    "\n",
    "# find words present in glove dataset\n",
    "words_in_glove = [w for w,is_true in zip(u_words_total,word_in_glove) if is_true]\n",
    "\n",
    "# find words not present in glove dataset\n",
    "words_not_in_glove = [w for w,is_true in zip(u_words_total,word_in_glove) if not is_true]\n",
    "\n",
    "# view how many unique words our text has\n",
    "print('Fraction of unique words in glove vectors: ', sum(word_in_glove)/len(word_in_glove))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique words are:  68459\n",
      "The first review looks like this: \n",
      "[4105, 10906, 5, 50300, 2813, 1223, 24, 0, 604, 477, 8, 5, 55, 235, 2386, 3001, 10, 1401, 1652, 1]\n",
      "And once this is converted back to words, it looks like: \n",
      "daniel greenfield a <Other> journalism fellow at the freedom center is a new york writer focusing on radical islam .\n"
     ]
    }
   ],
   "source": [
    "# create the dictionary\n",
    "word2num = dict(zip(words_in_glove,range(len(words_in_glove))))\n",
    "\n",
    "# find length of dic\n",
    "len_glove_words = len(word2num)\n",
    "\n",
    "# find frequent words in doc not present in glove dataset \n",
    "freq_words_not_glove = [w for w in words_not_in_glove if w in u_words_frequent]\n",
    "\n",
    "# add findings into dic\n",
    "b = dict(zip(freq_words_not_glove,range(len(word2num), len(word2num)+len(freq_words_not_glove))))\n",
    "\n",
    "# treat word2num and dict as the key-value pairs\n",
    "word2num = dict(**word2num, **b)\n",
    "\n",
    "#add len of new dict as feature \n",
    "word2num['<Other>'] = len(word2num)\n",
    "\n",
    "# add values and associated keys into new dict\n",
    "num2word = dict(zip(word2num.values(), word2num.keys()))\n",
    "\n",
    "# split and convert values to integers\n",
    "int_text = [[word2num[word] if word in word2num else word2num['<Other>'] \n",
    "             for word in content.split()] for content in df_train.text.values]\n",
    "\n",
    "# view process and dict properties\n",
    "print('The number of unique words are: ', len(u_words))\n",
    "print('The first review looks like this: ')\n",
    "print(int_text[0][:20])\n",
    "print('And once this is converted back to words, it looks like: ')\n",
    "print(' '.join([num2word[i] for i in int_text[0][:20]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### article length distrubution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of articles greater than 500 in length is:  2336\n",
      "The number of articles less than 50 in length is:  272\n"
     ]
    }
   ],
   "source": [
    "# plot distrubution of artibcles length\n",
    "plt.hist([len(t) for t in int_text],50)\n",
    "plt.show()\n",
    "# print output for nr of articles >500 & <50 in length\n",
    "print('The number of articles greater than 500 in length is: ', np.sum(np.array([len(t)>500 for t in int_text])))\n",
    "print('The number of articles less than 50 in length is: ', np.sum(np.array([len(t)<50 for t in int_text])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sequencing / prepadding\n",
    "We must pass the same lengths of sentences into our model. Therefore we will employ padding in the following way: \n",
    "- Sequences less than 500 in length will be prepadded\n",
    "- Sequences that are longer than 500 will be truncated. \n",
    "- We assume sentiment of fake vs real from the first 500 words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pad as len of word2num\n",
    "num2word[len(word2num)] = '<PAD>'\n",
    "# add pad as a feature in dict \n",
    "word2num['<PAD>'] = len(word2num)\n",
    "\n",
    "# execute padding as defined above\n",
    "for i, t in enumerate(int_text):\n",
    "    # length <500: prepadded  \n",
    "    if len(t)<500:\n",
    "        int_text[i] = [word2num['<PAD>']]*(500-len(t)) + t\n",
    "    # length >500: truncated  \n",
    "    elif len(t)>500:\n",
    "        int_text[i] = t[:500]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# assign x (text) and y (label) \n",
    "x = np.array(int_text)\n",
    "# fake = 0, real = 1\n",
    "y = (df_train.label.values=='REAL').astype('int')\n",
    "\n",
    "# split data into training (80%) and test/validation (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Many to One LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3569 samples, validate on 397 samples\n",
      "Epoch 1/3\n",
      "3569/3569 [==============================] - 39s 11ms/step - loss: 0.6693 - acc: 0.5901 - val_loss: 0.5587 - val_acc: 0.7078\n",
      "Epoch 2/3\n",
      "3569/3569 [==============================] - 35s 10ms/step - loss: 0.5489 - acc: 0.7534 - val_loss: 0.3944 - val_acc: 0.8363\n",
      "Epoch 3/3\n",
      "3569/3569 [==============================] - 36s 10ms/step - loss: 0.2432 - acc: 0.9123 - val_loss: 0.3639 - val_acc: 0.8564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16b996898>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model \n",
    "model = Sequential()\n",
    "# add embedding with deimension \n",
    "model.add(Embedding(input_dim=len(word2num),\n",
    "                    output_dim=32))\n",
    "# add LSTM layer \n",
    "model.add(LSTM(100))\n",
    "# add output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# define model compiler \n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# define model paramters\n",
    "batch_size = 64\n",
    "epochs = 3\n",
    "# fit model\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_test, y_test),\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397/397 [==============================] - 1s 3ms/step\n",
      "\n",
      "acc: 85.64%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, batch_size=64, verbose=1)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this section we have tried a lot of different approaches using nueral networks of lower and higher complexity. We have also explored other apporaches to text processning such as embedding. The results might not be better than the simpler models covered in section 2, perhaps due to the higher data necessity. The simplest models, sequentional nueral network produced the highest results and thus we'll try using it for our ensamble in the next section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
