{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Assignment\n",
    "**Authors**: Vilhelm Stiernstedt & Sharon Mar√≠n Salazar\n",
    "<br>\n",
    "**Date**: 20/05/2018\n",
    "\n",
    "### Description\n",
    "Classification problem of News Report (document) for classes (FAKE, REAL). Try text-related classifiers such as Naive Bayes, MaxEnt, SVM. Use NLTK+SKLearn, NLP Pre-processing, Classifiers and CV-evaluation.\n",
    "\n",
    "#### Dataset\n",
    "**fake_or_real_news_training:**\n",
    "- ID: ID of the tweet\n",
    "- Title: Title of the news report\n",
    "- Text: Textual content of the news report\n",
    "- Label: Target Variable [FAKE, REAL]\n",
    "- X1, X2 additional fields\n",
    "\n",
    "**fake_or_real_news_test:**\n",
    "- ID, title and text\n",
    "- Predict Label\n",
    "\n",
    "#### Advices\n",
    "- Take a look to the data\n",
    "- Try the pre-processing methodologies we have seen in class\n",
    "- TF-IDF seems to be better (but try it!)\n",
    "- N-grams pay the effort\n",
    "- Less than 90-92%? -> Try again\n",
    "\n",
    "#### Plan\n",
    "1. Variable analysis\n",
    "    - Features\n",
    "    - Other insight\n",
    "2. Data Processing\n",
    "    - Drop features\n",
    "    - Label\n",
    "3. Modelling\n",
    "    - Navie Bayes\n",
    "    - MaxEnt\n",
    "    - SVM\n",
    "4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.classify import MaxentClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import PipelineHelper # https://github.com/bmurauer/pipelinehelper/blob/master/pipelinehelper.py\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import warnings\n",
    "\n",
    "# download required nltk packages (NB. commented out)\n",
    "# nltk.download()\n",
    "\n",
    "# plot settings\n",
    "%matplotlib inline\n",
    "\n",
    "# pandas view settings -> see all contents of column\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# Warning settings -> suppress depreciation warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions\n",
    "### Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stop words - english\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# define lemmatizer for simple analysis \n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# create normalization function for analysis of title and text\n",
    "def normalizer(text):\n",
    "    clean_text = re.sub('[^\\x00-\\x7F]+', \"\", text) # remove non-ascii characters\n",
    "    clean_text = re.sub('(\\r)+', \"\",  clean_text) # remove newline characters\n",
    "    clean_text = re.sub(r'@([A-Za-z0-9_]+)', \"\",  clean_text) # remove twitter handles\n",
    "    clean_text = re.sub(r\"(https|http)\\S+\", \"\",  clean_text) # remove hyperlinks\n",
    "    clean_text = re.sub(\"[^a-zA-Z]\", \" \", clean_text) # remove all but letters remains\n",
    "    tokens = nltk.word_tokenize(clean_text)[2:] # tokenize words\n",
    "    lower_case = [l.lower() for l in tokens] # convert to lowercase\n",
    "    filtered_result = list(filter(lambda l: l not in stop_words, lower_case)) # filter stopwords\n",
    "    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result] # stem words with lemmatizer\n",
    "    return lemmas\n",
    "\n",
    "# define function to construct our ngrams for analysis of title and text\n",
    "def ngrams(input_list):\n",
    "    bigrams = [' '.join(t) for t in list(zip(input_list, input_list[1:]))]\n",
    "    trigrams = [' '.join(t) for t in list(zip(input_list, input_list[1:], input_list[2:]))]\n",
    "    quadgrams = [' '.join(t) for t in list(zip(input_list, input_list[1:], input_list[3:]))]\n",
    "    return bigrams+trigrams+quadgrams\n",
    "\n",
    "# define function to count words for analysis of ngrams (bi, tri, quad) for title and text\n",
    "def count_words(input):\n",
    "    cnt = collections.Counter()\n",
    "    for row in input:\n",
    "        for word in row:\n",
    "            cnt[word] += 1\n",
    "    return cnt\n",
    "\n",
    "# exclaimation counter for analysis of text (potentially introduce as new feautre)\n",
    "def exclaimation_counter(article):\n",
    "    nr_abs = article.count('!')\n",
    "    text_len = len(article)\n",
    "    nr_rel = nr_abs/text_len\n",
    "    return nr_rel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing\n",
    "#### Stemmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define count vectorizer for modelling (different parameter inputs will be given in modelling)\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# define Snowball stemmer (different parameter inputs will be given in modelling\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# define new vectorizer function with snowball stemmer\n",
    "class SnowballCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(SnowballCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([snowball_stemmer.stem(w) for w in analyzer(doc)])\n",
    "    \n",
    "# define new vectorizer function with Porter stemmer NLTK exten \n",
    "# (different parameter inputs will be given in modelling)\n",
    "porter_stemmer = PorterStemmer(mode='NLTK_EXTENSIONS')\n",
    "\n",
    "# define new vectorizer function with porter stemmer\n",
    "class PorterCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(PorterCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([porter_stemmer.stem(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build pipeline with multiple models\n",
    "# https://github.com/bmurauer/pipelinehelper/blob/master/pipelinehelper.py\n",
    "\n",
    "from sklearn.base import TransformerMixin, BaseEstimator, ClassifierMixin\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "class PipelineHelper(BaseEstimator, TransformerMixin, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, available_models=None, selected_model=None, include_bypass=False):\n",
    "        self.include_bypass = include_bypass\n",
    "        self.selected_model = selected_model\n",
    "        # this is required for the clone operator used in gridsearch\n",
    "        if type(available_models) == dict:\n",
    "            self.available_models = available_models\n",
    "        # this is the case for constructing the helper initially\n",
    "        else:\n",
    "            # a string identifier is required for assigning parameters\n",
    "            self.available_models = {}\n",
    "            for (key, model) in available_models:\n",
    "                self.available_models[key] = model\n",
    "\n",
    "    def generate(self, param_dict={}):\n",
    "        per_model_parameters = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "        # collect parameters for each specified model\n",
    "        for k, values in param_dict.items():\n",
    "            model_name = k.split('__')[0]\n",
    "            param_name = k[len(model_name)+2:]  # might be nested\n",
    "            if model_name not in self.available_models:\n",
    "                raise Exception('no such model: {0}'.format(model_name))\n",
    "            per_model_parameters[model_name][param_name] = values\n",
    "\n",
    "        ret = []\n",
    "\n",
    "        # create instance for cartesion product of all available parameters for each model\n",
    "        for model_name, param_dict in per_model_parameters.items():\n",
    "            parameter_sets = (dict(zip(param_dict, x)) for x in itertools.product(*param_dict.values()))\n",
    "            for parameters in parameter_sets:\n",
    "                ret.append((model_name, parameters))\n",
    "\n",
    "        # for every model that has no specified parameters, add the default model\n",
    "        for model_name in self.available_models.keys():\n",
    "            if model_name not in per_model_parameters:\n",
    "                ret.append((model_name, dict()))\n",
    "\n",
    "        # check if the stage is to be bypassed as one configuration\n",
    "        if self.include_bypass:\n",
    "            ret.append((None, dict(), True))\n",
    "        return ret\n",
    "\n",
    "    def get_params(self, deep=False):\n",
    "        return {'available_models': self.available_models,\n",
    "                'selected_model': self.selected_model,\n",
    "                'include_bypass': self.include_bypass}\n",
    "\n",
    "    def set_params(self, selected_model, available_models=None, include_bypass=False):\n",
    "        include_bypass = len(selected_model) == 3 and selected_model[2]\n",
    "\n",
    "        if available_models:\n",
    "            self.available_models = available_models\n",
    "\n",
    "        if selected_model[0] is None and include_bypass:\n",
    "            self.selected_model = None\n",
    "            self.include_bypass = True\n",
    "        else:\n",
    "            if selected_model[0] not in self.available_models:\n",
    "                raise Exception('so such model available: {0}'.format(selected_model[0]))\n",
    "            self.selected_model = self.available_models[selected_model[0]]\n",
    "            self.selected_model.set_params(**selected_model[1])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.selected_model is None and not self.include_bypass:\n",
    "            raise Exception('no model was set')\n",
    "        elif self.selected_model is None:\n",
    "            # print('bypassing model for fitting, returning self')\n",
    "            return self\n",
    "        else:\n",
    "            # print('using model for fitting: ', self.selected_model.__class__.__name__)\n",
    "            return self.selected_model.fit(X, y)\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if self.selected_model is None and not self.include_bypass:\n",
    "            raise Exception('no model was set')\n",
    "        elif self.selected_model is None:\n",
    "            # print('bypassing model for transforming:')\n",
    "            # print(X[:10])\n",
    "            return X\n",
    "        else:\n",
    "            # print('using model for transforming: ', self.selected_model.__class__.__name__)\n",
    "            return self.selected_model.transform(X)\n",
    "\n",
    "    def predict(self, x):\n",
    "        if self.include_bypass:\n",
    "            raise Exception('bypassing classifier is not allowed')\n",
    "        if self.selected_model is None:\n",
    "            raise Exception('no model was set')\n",
    "        return self.selected_model.predict(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to data\n",
    "data_path = 'data/'\n",
    "\n",
    "# load test and train\n",
    "df_train = pd.read_csv(data_path+'fake_or_real_news_training.csv')\n",
    "df_test = pd.read_csv(data_path+'fake_or_real_news_test.csv')\n",
    "\n",
    "# set index\n",
    "df_train.set_index('ID', inplace=True)\n",
    "df_test.set_index('ID', inplace=True)\n",
    "\n",
    "# define combined df\n",
    "all_data = df_train.append(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_train[['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save label\n",
    "label = df_train.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data and labels into train and validation 80/20\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(df_model, label,\n",
    "                                                                test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline (vectrizer, models)\n",
    "pipeline = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf', PipelineHelper([\n",
    "                            ('svm_clf', SGDClassifier()),\n",
    "                            ('multi_nb_clf', MultinomialNB()),\n",
    "                        ])),\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipline parameters\n",
    "parameters = {'vect__ngram_range': (1, 1),\n",
    "              'vect__stop_words': (None),\n",
    "              'vect__binary': (False),\n",
    "              'vect__lowercase': (False),\n",
    "              'vect__max_df': (1.0),\n",
    "              'vect__max_features': (None),\n",
    "              'vect__tokenizer': (None),\n",
    "              'clf__selected_model': pipeline.named_steps['clf'].generate({\n",
    "                  'svm_clf__loss': ('hinge', 'squared_hinge'),\n",
    "                  'svm_clf__alpha': (0.5, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6),\n",
    "                  'svm_clf__l1_ratio': (0, 0.1, 0.25, 0.5, 0.75, 1.0),\n",
    "                  'svm_clf__class_weight': ('balanced', None),\n",
    "                  'multi_nb_clf__alpha': (0.1, 0.25, 0.5, 0.75, 1),\n",
    "                })\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-979319ca6224>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# fit model based\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mrscv_clf_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrscv_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# get best score from CV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0;31m# Regenerate parameter iterable for each fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0mcandidate_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_param_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m         \u001b[0mn_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;31m# look up sampled parameter settings in parameter grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0mgrid_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgrid_size\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mproduct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         return sum(product(len(v) for v in p.values()) if p else 1\n\u001b[0;32m--> 124\u001b[0;31m                    for p in self.param_grid)\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mproduct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         return sum(product(len(v) for v in p.values()) if p else 1\n\u001b[0;32m--> 124\u001b[0;31m                    for p in self.param_grid)\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# Product function that can handle iterables (np.product can't).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mproduct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         return sum(product(len(v) for v in p.values()) if p else 1\n\u001b[0m\u001b[1;32m    124\u001b[0m                    for p in self.param_grid)\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# define random search grid with cv\n",
    "rscv_clf = RandomizedSearchCV(estimator=pipeline, verbose=1,\n",
    "                              param_distributions=parameters,\n",
    "                              n_jobs=3, n_iter=100, cv=3, \n",
    "                              random_state=42)\n",
    "\n",
    "# fit model based\n",
    "rscv_clf_mod = rscv_clf.fit(x_train, y_train)\n",
    "\n",
    "# get best score from CV\n",
    "rscv_clf_mod.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
